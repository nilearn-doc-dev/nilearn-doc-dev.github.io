<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

<meta property="og:title" content="9.4.4.1.5. Intro to GLM Analysis: a single-session, single-subject fMRI dataset" />
  
<meta property="og:type" content="website" />
  
<meta property="og:url" content="https://nilearn.github.io/auto_examples/00_tutorials/plot_single_subject_single_run.html" />
  
<meta property="og:site_name" content="Nilearn" />
  
<meta property="og:description" content="In this tutorial, we use a General Linear Model ( GLM) to compare the fMRI signal during periods of auditory stimulation versus periods of rest. The analyse described here is performed in the nativ..." />
  
<meta property="og:image" content="https://nilearn.github.io/_static/nilearn-logo.png" />
  
<meta property="og:image:alt" content="Nilearn" />
  <link rel="search" title="Search" href="../../search.html" /><link rel="next" title="9.4.4.2. Visualization of brain images" href="../01_plotting/index.html" /><link rel="prev" title="9.4.4.1.4. A introduction tutorial to fMRI decoding" href="plot_decoding_tutorial.html" />

    <link rel="shortcut icon" href="../../_static/favicon.ico"/><meta name="generator" content="sphinx-4.4.0, furo 2022.03.04"/>
        <title>9.4.4.1.5. Intro to GLM Analysis: a single-session, single-subject fMRI dataset - Nilearn</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=935aa2abcc5c1da4283d1dc201fb1f0add16d23a" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.59c74d8c95b765a7fd995ac71d459ebe.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=25ceb02ed1c46dc30f2321ff83e92799f69dfdb9" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #232629;
  --color-code-foreground: #cccccc;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #232629;
  --color-code-foreground: #cccccc;
  
      }
    }
  }
</style></head>
  <body>
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Nilearn</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/nilearn-transparent.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Nilearn</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../nistats_migration.html">A Quick Guide to Migrating Nistats Code to Nilearn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../authors.html">People</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../authors.html#citing-nilearn">Citing nilearn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../authors.html#citing-scikit-learn">Citing scikit-learn</a></li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../../user_guide.html">User guide: table of contents</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html">1. Introduction: nilearn in a nutshell</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../decoding/index.html">2. Decoding and MVPA: predicting from brain images</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../decoding/decoding_intro.html">2.1. An introduction to decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../decoding/estimator_choice.html">2.2. Choosing the right predictive model for neuroimaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../decoding/frem.html">2.3. FREM: fast ensembling of regularized models for robust decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../decoding/space_net.html">2.4. SpaceNet: decoding with spatial structure for better maps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../decoding/searchlight.html">2.5. Searchlight : finding voxels containing information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../decoding/going_further.html">2.6. Running scikit-learn functions for more control on the analysis</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../connectivity/index.html">3. Functional connectivity and resting state</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../connectivity/functional_connectomes.html">3.1. Extracting times series to build a functional connectome</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../connectivity/connectome_extraction.html">3.2. Connectome extraction: inverse covariance for direct connections</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../developers/group_sparse_covariance.html">3.2.3.1. Group-sparse covariance estimation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../connectivity/resting_state_networks.html">3.3. Extracting functional brain networks: ICA and related</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../connectivity/region_extraction.html">3.4. Region Extraction for better brain parcellations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../connectivity/parcellating.html">3.5. Clustering to parcellate the brain in regions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../plotting/index.html">4. Plotting brain images</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../glm/index.html">5. Analyzing fMRI using GLMs</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../glm/glm_intro.html">5.1. An introduction to GLMs in fMRI statistical analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../glm/first_level_model.html">5.2. First level models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../glm/second_level_model.html">5.3. Second level models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../manipulating_images/index.html">6. Manipulation brain volumes with nilearn</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../manipulating_images/input_output.html">6.1. Input and output: neuroimaging data representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../manipulating_images/manipulating_images.html">6.2. Manipulating images: resampling, smoothing, masking, ROIs…</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../manipulating_images/masker_objects.html">6.3. From neuroimaging volumes to data matrices: the masker objects</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../building_blocks/index.html">7. Advanced usage: manual pipelines and scaling up</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../building_blocks/manual_pipeline.html">7.1. Building your own neuroimaging machine-learning pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../building_blocks/neurovault.html">7.2. Downloading statistical maps from the Neurovault repository</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../modules/reference.html">8. Reference documentation: all nilearn functions</a></li>
<li class="toctree-l2 current has-children"><a class="reference internal" href="../index.html">9. Examples</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3 current has-children"><a class="reference internal" href="index.html">9.4.4.1. Basic tutorials</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="plot_python_101.html">9.4.4.1.1. Basic numerics and plotting with Python</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_nilearn_101.html">9.4.4.1.2. Basic nilearn example: manipulating and looking at data</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_3d_and_4d_niimg.html">9.4.4.1.3. 3D and 4D niimgs: handling and visualizing</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_decoding_tutorial.html">9.4.4.1.4. A introduction tutorial to fMRI decoding</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">9.4.4.1.5. Intro to GLM Analysis: a single-session, single-subject fMRI dataset</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../01_plotting/index.html">9.4.4.2. Visualization of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_demo_glass_brain.html">9.4.4.2.1. Glass brain plotting in nilearn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_visualize_megatrawls_netmats.html">9.4.4.2.2. Visualizing Megatrawls Network Matrices from Human Connectome Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_atlas.html">9.4.4.2.3. Basic Atlas plotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_multiscale_parcellations.html">9.4.4.2.4. Visualizing multiscale functional brain parcellations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_colormaps.html">9.4.4.2.5. Matplotlib colormaps in Nilearn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_overlay.html">9.4.4.2.6. Visualizing a probabilistic atlas: the default mode in the MSDL atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_overlay.html#visualizing-a-probabilistic-atlas-with-plot-prob-atlas">9.4.4.2.7. Visualizing a probabilistic atlas with plot_prob_atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_dim_plotting.html">9.4.4.2.8. Controlling the contrast of the background when plotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_visualization.html">9.4.4.2.9. NeuroImaging volumes visualization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_carpet.html">9.4.4.2.10. Visualizing global patterns with a carpet plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_haxby_masks.html">9.4.4.2.11. Plot Haxby masks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_surface_projection_strategies.html">9.4.4.2.12. Technical point: Illustration of the volume to surface sampling schemes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_demo_plotting.html">9.4.4.2.13. Plotting tools in nilearn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_prob_atlas.html">9.4.4.2.14. Visualizing 4D probabilistic atlas maps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_surf_stat_map.html">9.4.4.2.15. Seed-based connectivity on the surface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_surf_atlas.html">9.4.4.2.16. Loading and plotting of a cortical surface atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_3d_map_to_surface_projection.html">9.4.4.2.17. Making a surface plot of a 3D statistical map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_demo_glass_brain_extensive.html">9.4.4.2.18. Glass brain plotting in nilearn (all options)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../01_plotting/plot_demo_more_plotting.html">9.4.4.2.19. More plotting tools from nilearn</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../02_decoding/index.html">9.4.4.3. Decoding and predicting from brain images</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_stimuli.html">9.4.4.3.1. Show stimuli of Haxby et al. dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_mixed_gambles_frem.html">9.4.4.3.2. FREM on Jimura et al “mixed gambles” dataset.</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_frem.html">9.4.4.3.3. Decoding with FREM: face vs house object recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_oasis_vbm_space_net.html">9.4.4.3.4. Voxel-Based Morphometry on Oasis dataset with Space-Net prior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_anova_svm.html">9.4.4.3.5. Decoding with ANOVA + SVM: face vs house in the Haxby dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_searchlight_surface.html">9.4.4.3.6. Cortical surface-based searchlight decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_multiclass.html">9.4.4.3.7. The haxby dataset: different multi-class strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_searchlight.html">9.4.4.3.8. Searchlight analysis of face vs house recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_glm_decoding.html">9.4.4.3.9. Decoding of a dataset after GLM fit for signal extraction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_grid_search.html">9.4.4.3.10. Setting a parameter by cross-validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_full_analysis.html">9.4.4.3.11. ROI-based decoding analysis in Haxby et al. dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_haxby_different_estimators.html">9.4.4.3.12. Different classifiers in decoding the Haxby dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_oasis_vbm.html">9.4.4.3.13. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_simulated_data.html">9.4.4.3.14. Example of pattern recognition on simulated data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_miyawaki_encoding.html">9.4.4.3.15. Encoding models for visual stimuli from Miyawaki et al. 2008</a></li>
<li class="toctree-l4"><a class="reference internal" href="../02_decoding/plot_miyawaki_reconstruction.html">9.4.4.3.16. Reconstruction of visual stimuli from Miyawaki et al. 2008</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../03_connectivity/index.html">9.4.4.4. Functional connectivity</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_probabilistic_atlas_extraction.html">9.4.4.4.1. Extracting signals of a probabilistic atlas of functional regions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_inverse_covariance_connectome.html">9.4.4.4.2. Computing a connectome with sparse inverse covariance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_simulated_connectome.html">9.4.4.4.3. Connectivity structure estimation on simulated data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_compare_decomposition.html">9.4.4.4.4. Deriving spatial maps from group fMRI data using ICA and Dictionary Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_seed_to_voxel_correlation.html">9.4.4.4.5. Producing single subject maps of seed-to-voxel correlation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_multi_subject_connectome.html">9.4.4.4.6. Group Sparse inverse covariance for multi-subject connectome</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_extract_regions_dictlearning_maps.html">9.4.4.4.7. Regions extraction using <span class="xref std std-term">Dictionary learning</span> and functional connectomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_atlas_comparison.html">9.4.4.4.8. Comparing connectomes on different reference atlases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_group_level_connectivity.html">9.4.4.4.9. Classification of age groups using functional connectivity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_signal_extraction.html">9.4.4.4.10. Extracting signals from a brain parcellation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_sphere_based_connectome.html">9.4.4.4.11. Extract signals on spheres and plot a connectome</a></li>
<li class="toctree-l4"><a class="reference internal" href="../03_connectivity/plot_data_driven_parcellations.html">9.4.4.4.12. Clustering methods to learn a brain parcellation from fMRI</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../04_glm_first_level/index.html">9.4.4.5. GLM: First level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_write_events_file.html">9.4.4.5.1. Generate an events.tsv file for the NeuroSpin localizer task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_fixed_effects.html">9.4.4.5.2. Example of explicit fixed effects fMRI model fitting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_adhd_dmn.html">9.4.4.5.3. Default Mode Network extraction of AHDH dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_design_matrix.html">9.4.4.5.4. Examples of design matrices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_fir_model.html">9.4.4.5.5. Analysis of an fMRI dataset with a Finite Impule Response (FIR) model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_spm_multimodal_faces.html">9.4.4.5.6. Single-subject data (two sessions) in native space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_hrf.html">9.4.4.5.7. Example of MRI response functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_fiac_analysis.html">9.4.4.5.8. Simple example of two-session fMRI model fitting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_bids_features.html">9.4.4.5.9. First level analysis of a complete BIDS dataset from openneuro</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_predictions_residuals.html">9.4.4.5.10. Predicted time series and residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_localizer_surface_analysis.html">9.4.4.5.11. Example of surface-based first-level analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../04_glm_first_level/plot_first_level_details.html">9.4.4.5.12. Understanding parameters of the first-level model</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../05_glm_second_level/index.html">9.4.4.6. GLM : Second level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../05_glm_second_level/plot_second_level_design_matrix.html">9.4.4.6.1. Example of second level design matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../05_glm_second_level/plot_proportion_activated_voxels.html">9.4.4.6.2. Second-level fMRI model: true positive proportion in clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../05_glm_second_level/plot_thresholding.html">9.4.4.6.3. Statistical testing of a second-level analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../05_glm_second_level/plot_oasis.html">9.4.4.6.4. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../05_glm_second_level/plot_second_level_two_sample_test.html">9.4.4.6.5. Second-level fMRI model: two-sample test, unpaired and paired</a></li>
<li class="toctree-l4"><a class="reference internal" href="../05_glm_second_level/plot_second_level_one_sample_test.html">9.4.4.6.6. Second-level fMRI model: one sample test</a></li>
<li class="toctree-l4"><a class="reference internal" href="../05_glm_second_level/plot_second_level_association_test.html">9.4.4.6.7. Example of generic design in second-level models</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../06_manipulating_images/index.html">9.4.4.7. Manipulating brain image volumes</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_negate_image.html">9.4.4.7.1. Negating an image with math_img</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_compare_mean_image.html">9.4.4.7.2. Comparing the means of 2 images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_smooth_mean_image.html">9.4.4.7.3. Smoothing an image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_extract_rois_smith_atlas.html">9.4.4.7.4. Regions Extraction of Default Mode Networks using Smith Atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_extract_regions_labels_image.html">9.4.4.7.5. Breaking an atlas of labels in separated regions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_resample_to_template.html">9.4.4.7.6. Resample an image to a template</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_nifti_simple.html">9.4.4.7.7. Simple example of NiftiMasker use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_nifti_labels_simple.html">9.4.4.7.8. Extracting signals from brain regions using the NiftiLabelsMasker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_extract_rois_statistical_maps.html">9.4.4.7.9. Region Extraction using a t-statistical map (3D)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_mask_computation.html">9.4.4.7.10. Understanding NiftiMasker and mask computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_affine_transformation.html">9.4.4.7.11. Visualization of affine resamplings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../06_manipulating_images/plot_roi_extraction.html">9.4.4.7.12. Computing a Region of Interest (ROI) mask manually</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../07_advanced/index.html">9.4.4.8. Advanced statistical analysis of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_ica_resting_state.html">9.4.4.8.1. Multivariate decompositions: Independent component analysis of fMRI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_localizer_simple_analysis.html">9.4.4.8.2. Massively univariate analysis of a calculation task from the Localizer dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_bids_analysis.html">9.4.4.8.3. BIDS dataset first and second level analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_age_group_prediction_cross_val.html">9.4.4.8.4. Functional connectivity predicts age group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_neurovault_meta_analysis.html">9.4.4.8.5. NeuroVault meta-analysis of stop-go paradigm studies.</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_surface_bids_analysis.html">9.4.4.8.6. Surface-based dataset first and second level analysis of a dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_localizer_mass_univariate_methods.html">9.4.4.8.7. Massively univariate analysis of a motor task from the Localizer dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_ica_neurovault.html">9.4.4.8.8. NeuroVault cross-study ICA maps.</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_haxby_mass_univariate.html">9.4.4.8.9. Massively univariate analysis of face vs house recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../07_advanced/plot_advanced_decoding_scikit.html">9.4.4.8.10. Advanced decoding using scikit learn</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../index.html">9. Examples</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">9.4.4.1. Basic tutorials</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_python_101.html">9.4.4.1.1. Basic numerics and plotting with Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_nilearn_101.html">9.4.4.1.2. Basic nilearn example: manipulating and looking at data</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_3d_and_4d_niimg.html">9.4.4.1.3. 3D and 4D niimgs: handling and visualizing</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_decoding_tutorial.html">9.4.4.1.4. A introduction tutorial to fMRI decoding</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">9.4.4.1.5. Intro to GLM Analysis: a single-session, single-subject fMRI dataset</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../01_plotting/index.html">9.4.4.2. Visualization of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_demo_glass_brain.html">9.4.4.2.1. Glass brain plotting in nilearn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_visualize_megatrawls_netmats.html">9.4.4.2.2. Visualizing Megatrawls Network Matrices from Human Connectome Project</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_atlas.html">9.4.4.2.3. Basic Atlas plotting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_multiscale_parcellations.html">9.4.4.2.4. Visualizing multiscale functional brain parcellations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_colormaps.html">9.4.4.2.5. Matplotlib colormaps in Nilearn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_overlay.html">9.4.4.2.6. Visualizing a probabilistic atlas: the default mode in the MSDL atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_overlay.html#visualizing-a-probabilistic-atlas-with-plot-prob-atlas">9.4.4.2.7. Visualizing a probabilistic atlas with plot_prob_atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_dim_plotting.html">9.4.4.2.8. Controlling the contrast of the background when plotting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_visualization.html">9.4.4.2.9. NeuroImaging volumes visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_carpet.html">9.4.4.2.10. Visualizing global patterns with a carpet plot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_haxby_masks.html">9.4.4.2.11. Plot Haxby masks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_surface_projection_strategies.html">9.4.4.2.12. Technical point: Illustration of the volume to surface sampling schemes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_demo_plotting.html">9.4.4.2.13. Plotting tools in nilearn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_prob_atlas.html">9.4.4.2.14. Visualizing 4D probabilistic atlas maps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_surf_stat_map.html">9.4.4.2.15. Seed-based connectivity on the surface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_surf_atlas.html">9.4.4.2.16. Loading and plotting of a cortical surface atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_3d_map_to_surface_projection.html">9.4.4.2.17. Making a surface plot of a 3D statistical map</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_demo_glass_brain_extensive.html">9.4.4.2.18. Glass brain plotting in nilearn (all options)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../01_plotting/plot_demo_more_plotting.html">9.4.4.2.19. More plotting tools from nilearn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../02_decoding/index.html">9.4.4.3. Decoding and predicting from brain images</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_stimuli.html">9.4.4.3.1. Show stimuli of Haxby et al. dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_mixed_gambles_frem.html">9.4.4.3.2. FREM on Jimura et al “mixed gambles” dataset.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_frem.html">9.4.4.3.3. Decoding with FREM: face vs house object recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_oasis_vbm_space_net.html">9.4.4.3.4. Voxel-Based Morphometry on Oasis dataset with Space-Net prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_anova_svm.html">9.4.4.3.5. Decoding with ANOVA + SVM: face vs house in the Haxby dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_searchlight_surface.html">9.4.4.3.6. Cortical surface-based searchlight decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_multiclass.html">9.4.4.3.7. The haxby dataset: different multi-class strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_searchlight.html">9.4.4.3.8. Searchlight analysis of face vs house recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_glm_decoding.html">9.4.4.3.9. Decoding of a dataset after GLM fit for signal extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_grid_search.html">9.4.4.3.10. Setting a parameter by cross-validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_full_analysis.html">9.4.4.3.11. ROI-based decoding analysis in Haxby et al. dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_haxby_different_estimators.html">9.4.4.3.12. Different classifiers in decoding the Haxby dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_oasis_vbm.html">9.4.4.3.13. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_simulated_data.html">9.4.4.3.14. Example of pattern recognition on simulated data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_miyawaki_encoding.html">9.4.4.3.15. Encoding models for visual stimuli from Miyawaki et al. 2008</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02_decoding/plot_miyawaki_reconstruction.html">9.4.4.3.16. Reconstruction of visual stimuli from Miyawaki et al. 2008</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../03_connectivity/index.html">9.4.4.4. Functional connectivity</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_probabilistic_atlas_extraction.html">9.4.4.4.1. Extracting signals of a probabilistic atlas of functional regions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_inverse_covariance_connectome.html">9.4.4.4.2. Computing a connectome with sparse inverse covariance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_simulated_connectome.html">9.4.4.4.3. Connectivity structure estimation on simulated data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_compare_decomposition.html">9.4.4.4.4. Deriving spatial maps from group fMRI data using ICA and Dictionary Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_seed_to_voxel_correlation.html">9.4.4.4.5. Producing single subject maps of seed-to-voxel correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_multi_subject_connectome.html">9.4.4.4.6. Group Sparse inverse covariance for multi-subject connectome</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_extract_regions_dictlearning_maps.html">9.4.4.4.7. Regions extraction using <span class="xref std std-term">Dictionary learning</span> and functional connectomes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_atlas_comparison.html">9.4.4.4.8. Comparing connectomes on different reference atlases</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_group_level_connectivity.html">9.4.4.4.9. Classification of age groups using functional connectivity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_signal_extraction.html">9.4.4.4.10. Extracting signals from a brain parcellation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_sphere_based_connectome.html">9.4.4.4.11. Extract signals on spheres and plot a connectome</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03_connectivity/plot_data_driven_parcellations.html">9.4.4.4.12. Clustering methods to learn a brain parcellation from fMRI</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../04_glm_first_level/index.html">9.4.4.5. GLM: First level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_write_events_file.html">9.4.4.5.1. Generate an events.tsv file for the NeuroSpin localizer task</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_fixed_effects.html">9.4.4.5.2. Example of explicit fixed effects fMRI model fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_adhd_dmn.html">9.4.4.5.3. Default Mode Network extraction of AHDH dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_design_matrix.html">9.4.4.5.4. Examples of design matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_fir_model.html">9.4.4.5.5. Analysis of an fMRI dataset with a Finite Impule Response (FIR) model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_spm_multimodal_faces.html">9.4.4.5.6. Single-subject data (two sessions) in native space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_hrf.html">9.4.4.5.7. Example of MRI response functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_fiac_analysis.html">9.4.4.5.8. Simple example of two-session fMRI model fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_bids_features.html">9.4.4.5.9. First level analysis of a complete BIDS dataset from openneuro</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_predictions_residuals.html">9.4.4.5.10. Predicted time series and residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_localizer_surface_analysis.html">9.4.4.5.11. Example of surface-based first-level analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04_glm_first_level/plot_first_level_details.html">9.4.4.5.12. Understanding parameters of the first-level model</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../05_glm_second_level/index.html">9.4.4.6. GLM : Second level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" role="switch" type="checkbox"/><label for="toctree-checkbox-23"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../05_glm_second_level/plot_second_level_design_matrix.html">9.4.4.6.1. Example of second level design matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05_glm_second_level/plot_proportion_activated_voxels.html">9.4.4.6.2. Second-level fMRI model: true positive proportion in clusters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05_glm_second_level/plot_thresholding.html">9.4.4.6.3. Statistical testing of a second-level analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05_glm_second_level/plot_oasis.html">9.4.4.6.4. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05_glm_second_level/plot_second_level_two_sample_test.html">9.4.4.6.5. Second-level fMRI model: two-sample test, unpaired and paired</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05_glm_second_level/plot_second_level_one_sample_test.html">9.4.4.6.6. Second-level fMRI model: one sample test</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05_glm_second_level/plot_second_level_association_test.html">9.4.4.6.7. Example of generic design in second-level models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../06_manipulating_images/index.html">9.4.4.7. Manipulating brain image volumes</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" role="switch" type="checkbox"/><label for="toctree-checkbox-24"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_negate_image.html">9.4.4.7.1. Negating an image with math_img</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_compare_mean_image.html">9.4.4.7.2. Comparing the means of 2 images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_smooth_mean_image.html">9.4.4.7.3. Smoothing an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_extract_rois_smith_atlas.html">9.4.4.7.4. Regions Extraction of Default Mode Networks using Smith Atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_extract_regions_labels_image.html">9.4.4.7.5. Breaking an atlas of labels in separated regions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_resample_to_template.html">9.4.4.7.6. Resample an image to a template</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_nifti_simple.html">9.4.4.7.7. Simple example of NiftiMasker use</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_nifti_labels_simple.html">9.4.4.7.8. Extracting signals from brain regions using the NiftiLabelsMasker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_extract_rois_statistical_maps.html">9.4.4.7.9. Region Extraction using a t-statistical map (3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_mask_computation.html">9.4.4.7.10. Understanding NiftiMasker and mask computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_affine_transformation.html">9.4.4.7.11. Visualization of affine resamplings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06_manipulating_images/plot_roi_extraction.html">9.4.4.7.12. Computing a Region of Interest (ROI) mask manually</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../07_advanced/index.html">9.4.4.8. Advanced statistical analysis of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" role="switch" type="checkbox"/><label for="toctree-checkbox-25"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_ica_resting_state.html">9.4.4.8.1. Multivariate decompositions: Independent component analysis of fMRI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_localizer_simple_analysis.html">9.4.4.8.2. Massively univariate analysis of a calculation task from the Localizer dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_bids_analysis.html">9.4.4.8.3. BIDS dataset first and second level analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_age_group_prediction_cross_val.html">9.4.4.8.4. Functional connectivity predicts age group</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_neurovault_meta_analysis.html">9.4.4.8.5. NeuroVault meta-analysis of stop-go paradigm studies.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_surface_bids_analysis.html">9.4.4.8.6. Surface-based dataset first and second level analysis of a dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_localizer_mass_univariate_methods.html">9.4.4.8.7. Massively univariate analysis of a motor task from the Localizer dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_ica_neurovault.html">9.4.4.8.8. NeuroVault cross-study ICA maps.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_haxby_mass_univariate.html">9.4.4.8.9. Massively univariate analysis of face vs house recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07_advanced/plot_advanced_decoding_scikit.html">9.4.4.8.10. Advanced decoding using scikit learn</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html">What’s new</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#dev">0.9.1.dev</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id100">0.9.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id204">0.8.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id309">0.8.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id414">0.7.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id518">0.7.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id623">0.6.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id726">0.6.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id829">0.6.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#rc">0.6.0rc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#b0">0.6.0b0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#a0">0.6.0a0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id948">0.5.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1051">0.5.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1155">0.5.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1159">0.5.0 rc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#beta">0.5.0 beta</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#alpha">0.5.0 alpha</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1272">0.4.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1374">0.4.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1476">0.4.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1581">0.3.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1685">0.3.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1689">0.3.0 beta</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1793">0.2.6</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1896">0.2.5.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id1900">0.2.5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2002">0.2.4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2104">0.2.3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2205">0.2.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#v0-2-1">0.2.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2312">0.2.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2417">0.1.4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2518">0.1.3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2620">0.1.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2721">0.1.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes/whats_new.html#id2822">0.1.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../development.html">Nilearn development process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../maintenance.html">Nilearn maintenance process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container"><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-auto-examples-00-tutorials-plot-single-subject-single-run-py"><span class="std std-ref">here</span></a>
to download the full example code or to run this example in your browser via Binder</p>
</div>
<section class="sphx-glr-example-title" id="intro-to-glm-analysis-a-single-session-single-subject-fmri-dataset">
<span id="sphx-glr-auto-examples-00-tutorials-plot-single-subject-single-run-py"></span><h1><span class="section-number">9.4.4.1.5. </span>Intro to GLM Analysis: a single-session, single-subject fMRI dataset<a class="headerlink" href="#intro-to-glm-analysis-a-single-session-single-subject-fmri-dataset" title="Permalink to this headline">#</a></h1>
<p>In this tutorial, we use a General Linear Model (<a class="reference internal" href="../../glossary.html#term-GLM"><span class="xref std std-term">GLM</span></a>) to compare the
<a class="reference internal" href="../../glossary.html#term-fMRI"><span class="xref std std-term">fMRI</span></a> signal during periods of auditory stimulation versus periods of rest.</p>
<p>The analyse described here is performed in the native space, directly on the
original <a class="reference internal" href="../../glossary.html#term-EPI"><span class="xref std std-term">EPI</span></a> scans without any spatial or temporal preprocessing.
(More sensitive results would likely be obtained on the corrected,
spatially normalized and smoothed images).</p>
<section id="the-data">
<h2><span class="section-number">9.4.4.1.5.1. </span>The data<a class="headerlink" href="#the-data" title="Permalink to this headline">#</a></h2>
<p>The dataset comes from an experiment conducted at the FIL by Geraint Rees
under the direction of Karl Friston. It is provided by FIL methods
group which develops the SPM software.</p>
<p>According to SPM documentation, 96 scans were acquired (repetition time
<a class="reference internal" href="../../glossary.html#term-TR"><span class="xref std std-term">TR</span></a> = 7s) in one session. The paradigm consisted of alternating periods
of stimulation and rest, lasting 42s each (that is, for 6 scans). The session
started with a rest block.  Auditory stimulation consisted of bi-syllabic words
presented binaurally at a rate of 60 per minute. The functional data starts at scan
number 4, that is the image file <code class="docutils literal notranslate"><span class="pre">fM00223_004</span></code>.</p>
<p>The whole brain <a class="reference internal" href="../../glossary.html#term-BOLD"><span class="xref std std-term">BOLD</span></a>/<a class="reference internal" href="../../glossary.html#term-EPI"><span class="xref std std-term">EPI</span></a> images were acquired on a 2T Siemens
MAGNETOM Vision system. Each scan consisted of 64 contiguous slices (64x64x64
3mm x 3mm x 3mm <a class="reference internal" href="../../glossary.html#term-voxel"><span class="xref std std-term">voxels</span></a>). Acquisition of one scan took 6.05s, with the
scan to scan repeat time (<a class="reference internal" href="../../glossary.html#term-TR"><span class="xref std std-term">TR</span></a>) set arbitrarily to 7s.</p>
</section>
<section id="retrieving-the-data">
<h2><span class="section-number">9.4.4.1.5.2. </span>Retrieving the data<a class="headerlink" href="#retrieving-the-data" title="Permalink to this headline">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this tutorial, we load the data using a data downloading
function. To input your own data, you will need to provide
a list of paths to your own files in the <code class="docutils literal notranslate"><span class="pre">subject_data</span></code> variable.
These should abide to the Brain Imaging Data Structure (<a class="reference internal" href="../../glossary.html#term-BIDS"><span class="xref std std-term">BIDS</span></a>)
organization.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_spm_auditory</span>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch" title="sklearn.utils.Bunch" class="sphx-glr-backref-module-sklearn-utils sphx-glr-backref-type-py-function"><span class="n">subject_data</span></a> <span class="o">=</span> <span class="n">fetch_spm_auditory</span><span class="p">()</span>
<a href="https://docs.python.org/3.8/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">subject_data</span><span class="o">.</span><span class="n">func</span></a>  <span class="c1"># print the list of names of functional images</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>['/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_004.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_005.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_006.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_007.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_008.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_009.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_010.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_011.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_012.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_013.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_014.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_015.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_016.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_017.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_018.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_019.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_020.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_021.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_022.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_023.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_024.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_025.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_026.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_027.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_028.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_029.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_030.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_031.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_032.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_033.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_034.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_035.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_036.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_037.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_038.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_039.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_040.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_041.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_042.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_043.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_044.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_045.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_046.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_047.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_048.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_049.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_050.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_051.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_052.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_053.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_054.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_055.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_056.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_057.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_058.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_059.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_060.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_061.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_062.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_063.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_064.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_065.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_066.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_067.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_068.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_069.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_070.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_071.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_072.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_073.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_074.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_075.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_076.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_077.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_078.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_079.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_080.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_081.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_082.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_083.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_084.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_085.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_086.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_087.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_088.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_089.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_090.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_091.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_092.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_093.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_094.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_095.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_096.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_097.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_098.img', '/home/alexis/nilearn_data/spm_auditory/sub001/fM00223/fM00223_099.img']
</pre></div>
</div>
<p>We can display the first functional image and the subject’s anatomy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.plotting</span> <span class="kn">import</span> <span class="n">plot_stat_map</span><span class="p">,</span> <span class="n">plot_anat</span><span class="p">,</span> <span class="n">plot_img</span>
<span class="n">plot_img</span><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">subject_data</span><span class="o">.</span><span class="n">func</span></a><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">colorbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cbar_tick_format</span><span class="o">=</span><span class="s2">"</span><span class="si">%i</span><span class="s2">"</span><span class="p">)</span>
<span class="n">plot_anat</span><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">subject_data</span><span class="o">.</span><span class="n">anat</span></a><span class="p">,</span> <span class="n">colorbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cbar_tick_format</span><span class="o">=</span><span class="s2">"</span><span class="si">%i</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="plot single subject single run" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_single_subject_single_run_001.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_001.png"/></li>
<li><img alt="plot single subject single run" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_single_subject_single_run_002.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_002.png"/></li>
</ul>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;nilearn.plotting.displays._slicers.OrthoSlicer object at 0x7f9e824f9690&gt;
</pre></div>
</div>
<p>Next, we concatenate all the 3D <a class="reference internal" href="../../glossary.html#term-EPI"><span class="xref std std-term">EPI</span></a> image into a single 4D image,
then we average them in order to create a background
image that will be used to display the activations:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.image</span> <span class="kn">import</span> <span class="n">concat_imgs</span><span class="p">,</span> <a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a>
<a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fmri_img</span></a> <span class="o">=</span> <span class="n">concat_imgs</span><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">subject_data</span><span class="o">.</span><span class="n">func</span></a><span class="p">)</span>
<a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a> <span class="o">=</span> <a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fmri_img</span></a><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="specifying-the-experimental-paradigm">
<h2><span class="section-number">9.4.4.1.5.3. </span>Specifying the experimental paradigm<a class="headerlink" href="#specifying-the-experimental-paradigm" title="Permalink to this headline">#</a></h2>
<p>We must now provide a description of the experiment, that is, define the
timing of the auditory stimulation and rest periods. This is typically
provided in an events.tsv file. The path of this file is
provided in the dataset.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">events</span></a> <span class="o">=</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_table.html#pandas.read_table" title="pandas.read_table" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-function"><span class="n">pd</span><span class="o">.</span><span class="n">read_table</span></a><span class="p">(</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch" title="sklearn.utils.Bunch" class="sphx-glr-backref-module-sklearn-utils sphx-glr-backref-type-py-function"><span class="n">subject_data</span></a><span class="p">[</span><span class="s1">'events'</span><span class="p">])</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">events</span></a>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<div class="table-wrapper"><table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>onset</th>
<th>duration</th>
<th>trial_type</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>0.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>1</th>
<td>42.0</td>
<td>42.0</td>
<td>active</td>
</tr>
<tr>
<th>2</th>
<td>84.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>3</th>
<td>126.0</td>
<td>42.0</td>
<td>active</td>
</tr>
<tr>
<th>4</th>
<td>168.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>5</th>
<td>210.0</td>
<td>42.0</td>
<td>active</td>
</tr>
<tr>
<th>6</th>
<td>252.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>7</th>
<td>294.0</td>
<td>42.0</td>
<td>active</td>
</tr>
<tr>
<th>8</th>
<td>336.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>9</th>
<td>378.0</td>
<td>42.0</td>
<td>active</td>
</tr>
<tr>
<th>10</th>
<td>420.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>11</th>
<td>462.0</td>
<td>42.0</td>
<td>active</td>
</tr>
<tr>
<th>12</th>
<td>504.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>13</th>
<td>546.0</td>
<td>42.0</td>
<td>active</td>
</tr>
<tr>
<th>14</th>
<td>588.0</td>
<td>42.0</td>
<td>rest</td>
</tr>
<tr>
<th>15</th>
<td>630.0</td>
<td>42.0</td>
<td>active</td>
</tr>
</tbody>
</table></div>
</div>
</div>
<br/>
<br/></section>
<section id="performing-the-glm-analysis">
<h2><span class="section-number">9.4.4.1.5.4. </span>Performing the GLM analysis<a class="headerlink" href="#performing-the-glm-analysis" title="Permalink to this headline">#</a></h2>
<p>It is now time to create and estimate a <code class="docutils literal notranslate"><span class="pre">FirstLevelModel</span></code> object, that will generate the <em>design matrix</em> using the  information provided by the <code class="docutils literal notranslate"><span class="pre">events</span></code> object.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.glm.first_level</span> <span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator" title="sklearn.base.BaseEstimator" class="sphx-glr-backref-module-sklearn-base sphx-glr-backref-type-py-class"><span class="n">FirstLevelModel</span></a>
</pre></div>
</div>
<p>Parameters of the first-level model</p>
<ul class="simple">
<li><p>t_r=7(s) is the time of repetition of acquisitions</p></li>
<li><p>noise_model=’ar1’ specifies the noise covariance model: a lag-1 dependence</p></li>
<li><p>standardize=False means that we do not want to rescale the time series to mean 0, variance 1</p></li>
<li><p>hrf_model=’spm’ means that we rely on the SPM “canonical hrf” model (without time or dispersion derivatives)</p></li>
<li><p>drift_model=’cosine’ means that we model the signal drifts as slow oscillating time functions</p></li>
<li><p>high_pass=0.01(Hz) defines the cutoff frequency (inverse of the time period).</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fmri_glm</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator" title="sklearn.base.BaseEstimator" class="sphx-glr-backref-module-sklearn-base sphx-glr-backref-type-py-class"><span class="n">FirstLevelModel</span></a><span class="p">(</span><span class="n">t_r</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                           <span class="n">noise_model</span><span class="o">=</span><span class="s1">'ar1'</span><span class="p">,</span>
                           <span class="n">standardize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">hrf_model</span><span class="o">=</span><span class="s1">'spm'</span><span class="p">,</span>
                           <span class="n">drift_model</span><span class="o">=</span><span class="s1">'cosine'</span><span class="p">,</span>
                           <span class="n">high_pass</span><span class="o">=</span><span class="mf">.01</span><span class="p">)</span>
</pre></div>
</div>
<p>Now that we have specified the model, we can run it on the <a class="reference internal" href="../../glossary.html#term-fMRI"><span class="xref std std-term">fMRI</span></a> image</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fmri_glm</span> <span class="o">=</span> <span class="n">fmri_glm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fmri_img</span></a><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">events</span></a><span class="p">)</span>
</pre></div>
</div>
<p>One can inspect the design matrix (rows represent time, and
columns contain the predictors).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">design_matrix</span></a> <span class="o">=</span> <a href="https://docs.python.org/3.8/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">fmri_glm</span><span class="o">.</span><span class="n">design_matrices_</span></a><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Formally, we have taken the first design matrix, because the model is
implictily meant to for multiple runs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.plotting</span> <span class="kn">import</span> <span class="n">plot_design_matrix</span>
<span class="n">plot_design_matrix</span><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">design_matrix</span></a><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_003.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_003.png"/><p>Save the design matrix image to disk
first create a directory where you want to write the images</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outdir</span></a> <span class="o">=</span> <span class="s1">'results'</span>
<span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3.8/library/os.path.html#os.path.exists" title="os.path.exists" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outdir</span></a><span class="p">):</span>
    <a href="https://docs.python.org/3.8/library/os.html#os.mkdir" title="os.mkdir" class="sphx-glr-backref-module-os sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">mkdir</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outdir</span></a><span class="p">)</span>

<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <a href="https://docs.python.org/3.8/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">join</span></a>
<span class="n">plot_design_matrix</span><span class="p">(</span>
    <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">design_matrix</span></a><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><a href="https://docs.python.org/3.8/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">join</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outdir</span></a><span class="p">,</span> <span class="s1">'design_matrix.png'</span><span class="p">))</span>
</pre></div>
</div>
<p>The first column contains the expected response profile of regions which are
sensitive to the auditory stimulation.
Let’s plot this first column</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot" title="matplotlib.pyplot.plot" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span></a><span class="p">(</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">design_matrix</span></a><span class="p">[</span><span class="s1">'active'</span><span class="p">])</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html#matplotlib.pyplot.xlabel" title="matplotlib.pyplot.xlabel" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span></a><span class="p">(</span><span class="s1">'scan'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.title.html#matplotlib.pyplot.title" title="matplotlib.pyplot.title" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">title</span></a><span class="p">(</span><span class="s1">'Expected Auditory Response'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="Expected Auditory Response" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_004.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_004.png"/></section>
<section id="detecting-voxels-with-significant-effects">
<h2><span class="section-number">9.4.4.1.5.5. </span>Detecting voxels with significant effects<a class="headerlink" href="#detecting-voxels-with-significant-effects" title="Permalink to this headline">#</a></h2>
<p>To access the estimated coefficients (Betas of the <a class="reference internal" href="../../glossary.html#term-GLM"><span class="xref std std-term">GLM</span></a> model), we
created <a class="reference internal" href="../../glossary.html#term-contrast"><span class="xref std std-term">contrast</span></a> with a single ‘1’ in each of the columns: The role
of the <a class="reference internal" href="../../glossary.html#term-contrast"><span class="xref std std-term">contrast</span></a> is to select some columns of the model –and
potentially weight them– to study the associated statistics. So in
a nutshell, a contrast is a weighted combination of the estimated
effects.  Here we can define canonical contrasts that just consider
the two effects in isolation —let’s call them “conditions”—
then a <a class="reference internal" href="../../glossary.html#term-contrast"><span class="xref std std-term">contrast</span></a> that makes the difference between these conditions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array" title="numpy.array" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">array</span></a>
<a href="https://docs.python.org/3.8/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">conditions</span></a> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'active'</span><span class="p">:</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array" title="numpy.array" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">array</span></a><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
                     <span class="mf">0.</span><span class="p">]),</span>
    <span class="s1">'rest'</span><span class="p">:</span>   <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array" title="numpy.array" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">array</span></a><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
                     <span class="mf">0.</span><span class="p">]),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We can then compare the two conditions ‘active’ and ‘rest’ by
defining the corresponding <a class="reference internal" href="../../glossary.html#term-contrast"><span class="xref std std-term">contrast</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">active_minus_rest</span></a> <span class="o">=</span> <a href="https://docs.python.org/3.8/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">conditions</span></a><span class="p">[</span><span class="s1">'active'</span><span class="p">]</span> <span class="o">-</span> <a href="https://docs.python.org/3.8/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">conditions</span></a><span class="p">[</span><span class="s1">'rest'</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s look at it: plot the coefficients of the <a class="reference internal" href="../../glossary.html#term-contrast"><span class="xref std std-term">contrast</span></a>, indexed by
the names of the columns of the design matrix.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.plotting</span> <span class="kn">import</span> <span class="n">plot_contrast_matrix</span>
<span class="n">plot_contrast_matrix</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">active_minus_rest</span></a><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">design_matrix</span></a><span class="o">=</span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">design_matrix</span></a><span class="p">)</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_005.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_005.png"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:label='conditions'&gt;
</pre></div>
</div>
<p>Below, we compute the estimated effect. It is in <a class="reference internal" href="../../glossary.html#term-BOLD"><span class="xref std std-term">BOLD</span></a> signal unit,
but has no statistical guarantees, because it does not take into
account the associated variance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">eff_map</span></a> <span class="o">=</span> <span class="n">fmri_glm</span><span class="o">.</span><span class="n">compute_contrast</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">active_minus_rest</span></a><span class="p">,</span>
                                    <span class="n">output_type</span><span class="o">=</span><span class="s1">'effect_size'</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to get statistical significance, we form a t-statistic, and
directly convert it into z-scale. The z-scale means that the values
are scaled to match a standard Gaussian distribution (mean=0,
variance=1), across voxels, if there were no effects in the data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a> <span class="o">=</span> <span class="n">fmri_glm</span><span class="o">.</span><span class="n">compute_contrast</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">active_minus_rest</span></a><span class="p">,</span>
                                  <span class="n">output_type</span><span class="o">=</span><span class="s1">'z_score'</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="plot-thresholded-z-scores-map">
<h2><span class="section-number">9.4.4.1.5.6. </span>Plot thresholded z scores map<a class="headerlink" href="#plot-thresholded-z-scores-map" title="Permalink to this headline">#</a></h2>
<p>We display it on top of the average
functional image of the series (could be the anatomical image of the
subject).  We use arbitrarily a threshold of 3.0 in z-scale. We’ll
see later how to use corrected thresholds. We will show 3
axial views, with display_mode=’z’ and cut_coords=3.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_stat_map</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span>
              <span class="n">display_mode</span><span class="o">=</span><span class="s1">'z'</span><span class="p">,</span> <span class="n">cut_coords</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">black_bg</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s1">'Active minus Rest (Z&gt;3)'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_006.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_006.png"/><p>Statistical significance testing. One should worry about the
statistical validity of the procedure: here we used an arbitrary
threshold of 3.0 but the threshold should provide some guarantees on
the risk of false detections (aka type-1 errors in statistics).
One suggestion is to control the false positive rate (<a class="reference internal" href="../../glossary.html#term-FPR-correction"><span class="xref std std-term">fpr</span></a>, denoted by
alpha) at a certain level, e.g. 0.001: this means that there is 0.1% chance
of declaring an inactive <a class="reference internal" href="../../glossary.html#term-voxel"><span class="xref std std-term">voxel</span></a>, active.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.glm</span> <span class="kn">import</span> <span class="n">threshold_stats_img</span>
<a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">_</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a> <span class="o">=</span> <span class="n">threshold_stats_img</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.001</span><span class="p">,</span> <span class="n">height_control</span><span class="o">=</span><span class="s1">'fpr'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Uncorrected p&lt;0.001 threshold: </span><span class="si">%.3f</span><span class="s1">'</span> <span class="o">%</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">)</span>
<span class="n">plot_stat_map</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">,</span>
              <span class="n">display_mode</span><span class="o">=</span><span class="s1">'z'</span><span class="p">,</span> <span class="n">cut_coords</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">black_bg</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s1">'Active minus Rest (p&lt;0.001)'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_007.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_007.png"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Uncorrected p&lt;0.001 threshold: 3.291
</pre></div>
</div>
<p>The problem is that with this you expect 0.001 * n_voxels to show up
while they’re not active — tens to hundreds of voxels. A more
conservative solution is to control the family wise error rate,
i.e. the probability of making only one false detection, say at
5%. For that we use the so-called Bonferroni correction.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">_</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a> <span class="o">=</span> <span class="n">threshold_stats_img</span><span class="p">(</span>
    <a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span> <span class="n">height_control</span><span class="o">=</span><span class="s1">'bonferroni'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Bonferroni-corrected, p&lt;0.05 threshold: </span><span class="si">%.3f</span><span class="s1">'</span> <span class="o">%</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">)</span>
<span class="n">plot_stat_map</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">,</span>
              <span class="n">display_mode</span><span class="o">=</span><span class="s1">'z'</span><span class="p">,</span> <span class="n">cut_coords</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">black_bg</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s1">'Active minus Rest (p&lt;0.05, corrected)'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_008.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_008.png"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Bonferroni-corrected, p&lt;0.05 threshold: 4.934
</pre></div>
</div>
<p>This is quite conservative indeed!  A popular alternative is to
control the expected proportion of
false discoveries among detections. This is called the False
discovery rate.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">_</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a> <span class="o">=</span> <span class="n">threshold_stats_img</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span> <span class="n">height_control</span><span class="o">=</span><span class="s1">'fdr'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'False Discovery rate = 0.05 threshold: </span><span class="si">%.3f</span><span class="s1">'</span> <span class="o">%</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">)</span>
<span class="n">plot_stat_map</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">,</span>
              <span class="n">display_mode</span><span class="o">=</span><span class="s1">'z'</span><span class="p">,</span> <span class="n">cut_coords</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">black_bg</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s1">'Active minus Rest (fdr=0.05)'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_009.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_009.png"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>False Discovery rate = 0.05 threshold: 2.904
</pre></div>
</div>
<p>Finally people like to discard isolated voxels (aka “small
clusters”) from these images. It is possible to generate a
thresholded map with small clusters removed by providing a
cluster_threshold argument. Here clusters smaller than 10 voxels
will be discarded.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">clean_map</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a> <span class="o">=</span> <span class="n">threshold_stats_img</span><span class="p">(</span>
    <a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span> <span class="n">height_control</span><span class="o">=</span><span class="s1">'fdr'</span><span class="p">,</span> <span class="n">cluster_threshold</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plot_stat_map</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">clean_map</span></a><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">,</span>
              <span class="n">display_mode</span><span class="o">=</span><span class="s1">'z'</span><span class="p">,</span> <span class="n">cut_coords</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">black_bg</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s1">'Active minus Rest (fdr=0.05), clusters &gt; 10 voxels'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_010.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_010.png"/><p>We can save the effect and zscore maps to the disk.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://nipy.org/nibabel/reference/nibabel.filebasedimages.html#nibabel.filebasedimages.FileBasedImage.to_filename" title="nibabel.filebasedimages.FileBasedImage.to_filename" class="sphx-glr-backref-module-nibabel-filebasedimages sphx-glr-backref-type-py-method"><span class="n">z_map</span><span class="o">.</span><span class="n">to_filename</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">join</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outdir</span></a><span class="p">,</span> <span class="s1">'active_vs_rest_z_map.nii.gz'</span><span class="p">))</span>
<a href="https://nipy.org/nibabel/reference/nibabel.filebasedimages.html#nibabel.filebasedimages.FileBasedImage.to_filename" title="nibabel.filebasedimages.FileBasedImage.to_filename" class="sphx-glr-backref-module-nibabel-filebasedimages sphx-glr-backref-type-py-method"><span class="n">eff_map</span><span class="o">.</span><span class="n">to_filename</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">join</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outdir</span></a><span class="p">,</span> <span class="s1">'active_vs_rest_eff_map.nii.gz'</span><span class="p">))</span>
</pre></div>
</div>
<p>We can furthermore extract and report the found positions in a table.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.reporting</span> <span class="kn">import</span> <span class="n">get_clusters_table</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">table</span></a> <span class="o">=</span> <span class="n">get_clusters_table</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">stat_threshold</span><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">,</span>
                           <span class="n">cluster_threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">table</span></a>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/home/alexis/singbrain/repo/nilearn/nilearn/reporting/_get_clusters_table.py:212: UserWarning: Attention: No clusters with stat higher than 2.903531892142391
  warnings.warn(
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<div class="table-wrapper"><table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Cluster ID</th>
<th>X</th>
<th>Y</th>
<th>Z</th>
<th>Peak Stat</th>
<th>Cluster Size (mm3)</th>
</tr>
</thead>
<tbody>
</tbody>
</table></div>
</div>
</div>
<br/>
<br/><p>This table can be saved for future use.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv" title="pandas.DataFrame.to_csv" class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"><span class="n">table</span><span class="o">.</span><span class="n">to_csv</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/os.path.html#os.path.join" title="os.path.join" class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"><span class="n">join</span></a><span class="p">(</span><a href="https://docs.python.org/3.8/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">outdir</span></a><span class="p">,</span> <span class="s1">'table.csv'</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="performing-an-f-test">
<h2><span class="section-number">9.4.4.1.5.7. </span>Performing an F-test<a class="headerlink" href="#performing-an-f-test" title="Permalink to this headline">#</a></h2>
<p>“active vs rest” is a typical t test: condition versus
baseline. Another popular type of test is an F test in which one
seeks whether a certain combination of conditions (possibly two-,
three- or higher-dimensional) explains a significant proportion of
the signal.  Here one might for instance test which voxels are well
explained by the combination of the active and rest condition.</p>
<p>Specify the contrast and compute the corresponding map. Actually, the
contrast specification is done exactly the same way as for t-
contrasts.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">effects_of_interest</span></a> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.vstack.html#numpy.vstack" title="numpy.vstack" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">vstack</span></a><span class="p">((</span><a href="https://docs.python.org/3.8/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">conditions</span></a><span class="p">[</span><span class="s1">'active'</span><span class="p">],</span> <a href="https://docs.python.org/3.8/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">conditions</span></a><span class="p">[</span><span class="s1">'rest'</span><span class="p">]))</span>
<span class="n">plot_contrast_matrix</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">effects_of_interest</span></a><span class="p">,</span> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="pandas.core.frame.DataFrame" class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">design_matrix</span></a><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>

<a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a> <span class="o">=</span> <span class="n">fmri_glm</span><span class="o">.</span><span class="n">compute_contrast</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">effects_of_interest</span></a><span class="p">,</span>
                                  <span class="n">output_type</span><span class="o">=</span><span class="s1">'z_score'</span><span class="p">)</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_011.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_011.png"/><p>Note that the statistic has been converted to a z-variable, which
makes it easier to represent it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">clean_map</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a> <span class="o">=</span> <span class="n">threshold_stats_img</span><span class="p">(</span>
    <a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">z_map</span></a><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span> <span class="n">height_control</span><span class="o">=</span><span class="s1">'fdr'</span><span class="p">,</span> <span class="n">cluster_threshold</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plot_stat_map</span><span class="p">(</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">clean_map</span></a><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><a href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="nibabel.nifti1.Nifti1Image" class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mean_img</span></a><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="o">=</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64" title="numpy.float64" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">threshold</span></a><span class="p">,</span>
              <span class="n">display_mode</span><span class="o">=</span><span class="s1">'z'</span><span class="p">,</span> <span class="n">cut_coords</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">black_bg</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s1">'Effects of interest (fdr=0.05), clusters &gt; 10 voxels'</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre></div>
</div>
<img alt="plot single subject single run" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_single_subject_single_run_012.png" srcset="../../_images/sphx_glr_plot_single_subject_single_run_012.png"/><p>Oops, there is a lot of non-neural signal in there (ventricles, arteries)…</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  28.699 seconds)</p>
<p><strong>Estimated memory usage:</strong>  316 MB</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-00-tutorials-plot-single-subject-single-run-py">
<div class="binder-badge docutils container">
<a class="reference external image-reference" href="https://mybinder.org/v2/gh/nilearn/nilearn.github.io/main?filepath=examples/auto_examples/00_tutorials/plot_single_subject_single_run.ipynb"><img alt="Launch binder" src="../../_images/binder_badge_logo.svg" width="150px"/></a>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/c1e6ae6b80e32989e5f0c207fc3ed533/plot_single_subject_single_run.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_single_subject_single_run.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/6bb22acaf7194124966b6111dc3fca75/plot_single_subject_single_run.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_single_subject_single_run.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../01_plotting/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title"><span class="section-number">9.4.4.2. </span>Visualization of brain images</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_decoding_tutorial.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title"><span class="section-number">9.4.4.1.4. </span>A introduction tutorial to fMRI decoding</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; The nilearn developers 2010-2022
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">9.4.4.1.5. Intro to GLM Analysis: a single-session, single-subject fMRI dataset</a><ul>
<li><a class="reference internal" href="#the-data">9.4.4.1.5.1. The data</a></li>
<li><a class="reference internal" href="#retrieving-the-data">9.4.4.1.5.2. Retrieving the data</a></li>
<li><a class="reference internal" href="#specifying-the-experimental-paradigm">9.4.4.1.5.3. Specifying the experimental paradigm</a></li>
<li><a class="reference internal" href="#performing-the-glm-analysis">9.4.4.1.5.4. Performing the GLM analysis</a></li>
<li><a class="reference internal" href="#detecting-voxels-with-significant-effects">9.4.4.1.5.5. Detecting voxels with significant effects</a></li>
<li><a class="reference internal" href="#plot-thresholded-z-scores-map">9.4.4.1.5.6. Plot thresholded z scores map</a></li>
<li><a class="reference internal" href="#performing-an-f-test">9.4.4.1.5.7. Performing an F-test</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    </body>
</html>