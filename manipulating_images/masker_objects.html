<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

<meta property="og:title" content="6.3. From neuroimaging volumes to data matrices: the masker objects" />
  
<meta property="og:type" content="website" />
  
<meta property="og:url" content="https://nilearn.github.io/manipulating_images/masker_objects.html" />
  
<meta property="og:site_name" content="Nilearn" />
  
<meta property="og:description" content="This chapter introduces the maskers: objects that go from neuroimaging volumes, on the disk or in memory, to data matrices, eg of time series. Chapters contents: The concept of “masker” objects, Ni..." />
  
<meta property="og:image" content="https://nilearn.github.io/images/niimgs.jpg" />
  
<meta property="og:image:alt" content="niimgs" />
  <link rel="search" title="Search" href="../search.html" /><link rel="next" title="7. Advanced usage: manual pipelines and scaling up" href="../building_blocks/index.html" /><link rel="prev" title="6.2. Manipulating images: resampling, smoothing, masking, ROIs…" href="manipulating_images.html" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/><meta name="generator" content="sphinx-4.4.0, furo 2022.03.04"/>
        <title>6.3. From neuroimaging volumes to data matrices: the masker objects - Nilearn</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=935aa2abcc5c1da4283d1dc201fb1f0add16d23a" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.59c74d8c95b765a7fd995ac71d459ebe.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=25ceb02ed1c46dc30f2321ff83e92799f69dfdb9" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #232629;
  --color-code-foreground: #cccccc;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #232629;
  --color-code-foreground: #cccccc;
  
      }
    }
  }
</style></head>
  <body>
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Nilearn</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/nilearn-transparent.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Nilearn</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../nistats_migration.html">A Quick Guide to Migrating Nistats Code to Nilearn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">People</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html#citing-nilearn">Citing nilearn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html#citing-scikit-learn">Citing scikit-learn</a></li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../user_guide.html">User guide: table of contents</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../introduction.html">1. Introduction: nilearn in a nutshell</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../decoding/index.html">2. Decoding and MVPA: predicting from brain images</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../decoding/decoding_intro.html">2.1. An introduction to decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../decoding/estimator_choice.html">2.2. Choosing the right predictive model for neuroimaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../decoding/frem.html">2.3. FREM: fast ensembling of regularized models for robust decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../decoding/space_net.html">2.4. SpaceNet: decoding with spatial structure for better maps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../decoding/searchlight.html">2.5. Searchlight : finding voxels containing information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../decoding/going_further.html">2.6. Running scikit-learn functions for more control on the analysis</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../connectivity/index.html">3. Functional connectivity and resting state</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../connectivity/functional_connectomes.html">3.1. Extracting times series to build a functional connectome</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../connectivity/connectome_extraction.html">3.2. Connectome extraction: inverse covariance for direct connections</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../developers/group_sparse_covariance.html">3.2.3.1. Group-sparse covariance estimation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../connectivity/resting_state_networks.html">3.3. Extracting functional brain networks: ICA and related</a></li>
<li class="toctree-l3"><a class="reference internal" href="../connectivity/region_extraction.html">3.4. Region Extraction for better brain parcellations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../connectivity/parcellating.html">3.5. Clustering to parcellate the brain in regions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../plotting/index.html">4. Plotting brain images</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../glm/index.html">5. Analyzing fMRI using GLMs</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/glm_intro.html">5.1. An introduction to GLMs in fMRI statistical analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/first_level_model.html">5.2. First level models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/second_level_model.html">5.3. Second level models</a></li>
</ul>
</li>
<li class="toctree-l2 current has-children"><a class="reference internal" href="index.html">6. Manipulation brain volumes with nilearn</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="input_output.html">6.1. Input and output: neuroimaging data representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="manipulating_images.html">6.2. Manipulating images: resampling, smoothing, masking, ROIs…</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">6.3. From neuroimaging volumes to data matrices: the masker objects</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../building_blocks/index.html">7. Advanced usage: manual pipelines and scaling up</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../building_blocks/manual_pipeline.html">7.1. Building your own neuroimaging machine-learning pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../building_blocks/neurovault.html">7.2. Downloading statistical maps from the Neurovault repository</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../modules/reference.html">8. Reference documentation: all nilearn functions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/index.html">9. Examples</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/00_tutorials/index.html">9.4.4.1. Basic tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/00_tutorials/plot_python_101.html">9.4.4.1.1. Basic numerics and plotting with Python</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/00_tutorials/plot_nilearn_101.html">9.4.4.1.2. Basic nilearn example: manipulating and looking at data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/00_tutorials/plot_3d_and_4d_niimg.html">9.4.4.1.3. 3D and 4D niimgs: handling and visualizing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/00_tutorials/plot_decoding_tutorial.html">9.4.4.1.4. A introduction tutorial to fMRI decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/00_tutorials/plot_single_subject_single_run.html">9.4.4.1.5. Intro to GLM Analysis: a single-session, single-subject fMRI dataset</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/01_plotting/index.html">9.4.4.2. Visualization of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_glass_brain.html">9.4.4.2.1. Glass brain plotting in nilearn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_visualize_megatrawls_netmats.html">9.4.4.2.2. Visualizing Megatrawls Network Matrices from Human Connectome Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_atlas.html">9.4.4.2.3. Basic Atlas plotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_multiscale_parcellations.html">9.4.4.2.4. Visualizing multiscale functional brain parcellations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_colormaps.html">9.4.4.2.5. Matplotlib colormaps in Nilearn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_overlay.html">9.4.4.2.6. Visualizing a probabilistic atlas: the default mode in the MSDL atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_overlay.html#visualizing-a-probabilistic-atlas-with-plot-prob-atlas">9.4.4.2.7. Visualizing a probabilistic atlas with plot_prob_atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_dim_plotting.html">9.4.4.2.8. Controlling the contrast of the background when plotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_visualization.html">9.4.4.2.9. NeuroImaging volumes visualization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_carpet.html">9.4.4.2.10. Visualizing global patterns with a carpet plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_haxby_masks.html">9.4.4.2.11. Plot Haxby masks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_surface_projection_strategies.html">9.4.4.2.12. Technical point: Illustration of the volume to surface sampling schemes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_plotting.html">9.4.4.2.13. Plotting tools in nilearn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_prob_atlas.html">9.4.4.2.14. Visualizing 4D probabilistic atlas maps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_surf_stat_map.html">9.4.4.2.15. Seed-based connectivity on the surface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_surf_atlas.html">9.4.4.2.16. Loading and plotting of a cortical surface atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_3d_map_to_surface_projection.html">9.4.4.2.17. Making a surface plot of a 3D statistical map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_glass_brain_extensive.html">9.4.4.2.18. Glass brain plotting in nilearn (all options)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_more_plotting.html">9.4.4.2.19. More plotting tools from nilearn</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/02_decoding/index.html">9.4.4.3. Decoding and predicting from brain images</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_stimuli.html">9.4.4.3.1. Show stimuli of Haxby et al. dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_mixed_gambles_frem.html">9.4.4.3.2. FREM on Jimura et al “mixed gambles” dataset.</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_frem.html">9.4.4.3.3. Decoding with FREM: face vs house object recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_oasis_vbm_space_net.html">9.4.4.3.4. Voxel-Based Morphometry on Oasis dataset with Space-Net prior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_anova_svm.html">9.4.4.3.5. Decoding with ANOVA + SVM: face vs house in the Haxby dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_searchlight_surface.html">9.4.4.3.6. Cortical surface-based searchlight decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_multiclass.html">9.4.4.3.7. The haxby dataset: different multi-class strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_searchlight.html">9.4.4.3.8. Searchlight analysis of face vs house recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_glm_decoding.html">9.4.4.3.9. Decoding of a dataset after GLM fit for signal extraction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_grid_search.html">9.4.4.3.10. Setting a parameter by cross-validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html">9.4.4.3.11. ROI-based decoding analysis in Haxby et al. dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_different_estimators.html">9.4.4.3.12. Different classifiers in decoding the Haxby dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_oasis_vbm.html">9.4.4.3.13. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_simulated_data.html">9.4.4.3.14. Example of pattern recognition on simulated data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_miyawaki_encoding.html">9.4.4.3.15. Encoding models for visual stimuli from Miyawaki et al. 2008</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/02_decoding/plot_miyawaki_reconstruction.html">9.4.4.3.16. Reconstruction of visual stimuli from Miyawaki et al. 2008</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/03_connectivity/index.html">9.4.4.4. Functional connectivity</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_probabilistic_atlas_extraction.html">9.4.4.4.1. Extracting signals of a probabilistic atlas of functional regions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_inverse_covariance_connectome.html">9.4.4.4.2. Computing a connectome with sparse inverse covariance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_simulated_connectome.html">9.4.4.4.3. Connectivity structure estimation on simulated data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_compare_decomposition.html">9.4.4.4.4. Deriving spatial maps from group fMRI data using ICA and Dictionary Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_seed_to_voxel_correlation.html">9.4.4.4.5. Producing single subject maps of seed-to-voxel correlation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_multi_subject_connectome.html">9.4.4.4.6. Group Sparse inverse covariance for multi-subject connectome</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_extract_regions_dictlearning_maps.html">9.4.4.4.7. Regions extraction using <span class="xref std std-term">Dictionary learning</span> and functional connectomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_atlas_comparison.html">9.4.4.4.8. Comparing connectomes on different reference atlases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_group_level_connectivity.html">9.4.4.4.9. Classification of age groups using functional connectivity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_signal_extraction.html">9.4.4.4.10. Extracting signals from a brain parcellation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_sphere_based_connectome.html">9.4.4.4.11. Extract signals on spheres and plot a connectome</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/03_connectivity/plot_data_driven_parcellations.html">9.4.4.4.12. Clustering methods to learn a brain parcellation from fMRI</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/04_glm_first_level/index.html">9.4.4.5. GLM: First level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_write_events_file.html">9.4.4.5.1. Generate an events.tsv file for the NeuroSpin localizer task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_fixed_effects.html">9.4.4.5.2. Example of explicit fixed effects fMRI model fitting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_adhd_dmn.html">9.4.4.5.3. Default Mode Network extraction of AHDH dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_design_matrix.html">9.4.4.5.4. Examples of design matrices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_fir_model.html">9.4.4.5.5. Analysis of an fMRI dataset with a Finite Impule Response (FIR) model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_spm_multimodal_faces.html">9.4.4.5.6. Single-subject data (two sessions) in native space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_hrf.html">9.4.4.5.7. Example of MRI response functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_fiac_analysis.html">9.4.4.5.8. Simple example of two-session fMRI model fitting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_bids_features.html">9.4.4.5.9. First level analysis of a complete BIDS dataset from openneuro</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_predictions_residuals.html">9.4.4.5.10. Predicted time series and residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_localizer_surface_analysis.html">9.4.4.5.11. Example of surface-based first-level analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_first_level_details.html">9.4.4.5.12. Understanding parameters of the first-level model</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/05_glm_second_level/index.html">9.4.4.6. GLM : Second level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_design_matrix.html">9.4.4.6.1. Example of second level design matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_proportion_activated_voxels.html">9.4.4.6.2. Second-level fMRI model: true positive proportion in clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_thresholding.html">9.4.4.6.3. Statistical testing of a second-level analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_oasis.html">9.4.4.6.4. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_two_sample_test.html">9.4.4.6.5. Second-level fMRI model: two-sample test, unpaired and paired</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_one_sample_test.html">9.4.4.6.6. Second-level fMRI model: one sample test</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_association_test.html">9.4.4.6.7. Example of generic design in second-level models</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/06_manipulating_images/index.html">9.4.4.7. Manipulating brain image volumes</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" role="switch" type="checkbox"/><label for="toctree-checkbox-15"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_negate_image.html">9.4.4.7.1. Negating an image with math_img</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_compare_mean_image.html">9.4.4.7.2. Comparing the means of 2 images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_smooth_mean_image.html">9.4.4.7.3. Smoothing an image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_extract_rois_smith_atlas.html">9.4.4.7.4. Regions Extraction of Default Mode Networks using Smith Atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_extract_regions_labels_image.html">9.4.4.7.5. Breaking an atlas of labels in separated regions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_resample_to_template.html">9.4.4.7.6. Resample an image to a template</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_nifti_simple.html">9.4.4.7.7. Simple example of NiftiMasker use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_nifti_labels_simple.html">9.4.4.7.8. Extracting signals from brain regions using the NiftiLabelsMasker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_extract_rois_statistical_maps.html">9.4.4.7.9. Region Extraction using a t-statistical map (3D)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_mask_computation.html">9.4.4.7.10. Understanding NiftiMasker and mask computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_affine_transformation.html">9.4.4.7.11. Visualization of affine resamplings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_roi_extraction.html">9.4.4.7.12. Computing a Region of Interest (ROI) mask manually</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../auto_examples/07_advanced/index.html">9.4.4.8. Advanced statistical analysis of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" role="switch" type="checkbox"/><label for="toctree-checkbox-16"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_ica_resting_state.html">9.4.4.8.1. Multivariate decompositions: Independent component analysis of fMRI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_localizer_simple_analysis.html">9.4.4.8.2. Massively univariate analysis of a calculation task from the Localizer dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_bids_analysis.html">9.4.4.8.3. BIDS dataset first and second level analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_age_group_prediction_cross_val.html">9.4.4.8.4. Functional connectivity predicts age group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_neurovault_meta_analysis.html">9.4.4.8.5. NeuroVault meta-analysis of stop-go paradigm studies.</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_surface_bids_analysis.html">9.4.4.8.6. Surface-based dataset first and second level analysis of a dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_localizer_mass_univariate_methods.html">9.4.4.8.7. Massively univariate analysis of a motor task from the Localizer dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_ica_neurovault.html">9.4.4.8.8. NeuroVault cross-study ICA maps.</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_haxby_mass_univariate.html">9.4.4.8.9. Massively univariate analysis of face vs house recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../auto_examples/07_advanced/plot_advanced_decoding_scikit.html">9.4.4.8.10. Advanced decoding using scikit learn</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_examples/index.html">9. Examples</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" role="switch" type="checkbox"/><label for="toctree-checkbox-17"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/00_tutorials/index.html">9.4.4.1. Basic tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" role="switch" type="checkbox"/><label for="toctree-checkbox-18"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_tutorials/plot_python_101.html">9.4.4.1.1. Basic numerics and plotting with Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_tutorials/plot_nilearn_101.html">9.4.4.1.2. Basic nilearn example: manipulating and looking at data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_tutorials/plot_3d_and_4d_niimg.html">9.4.4.1.3. 3D and 4D niimgs: handling and visualizing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_tutorials/plot_decoding_tutorial.html">9.4.4.1.4. A introduction tutorial to fMRI decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_tutorials/plot_single_subject_single_run.html">9.4.4.1.5. Intro to GLM Analysis: a single-session, single-subject fMRI dataset</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/01_plotting/index.html">9.4.4.2. Visualization of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" role="switch" type="checkbox"/><label for="toctree-checkbox-19"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_glass_brain.html">9.4.4.2.1. Glass brain plotting in nilearn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_visualize_megatrawls_netmats.html">9.4.4.2.2. Visualizing Megatrawls Network Matrices from Human Connectome Project</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_atlas.html">9.4.4.2.3. Basic Atlas plotting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_multiscale_parcellations.html">9.4.4.2.4. Visualizing multiscale functional brain parcellations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_colormaps.html">9.4.4.2.5. Matplotlib colormaps in Nilearn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_overlay.html">9.4.4.2.6. Visualizing a probabilistic atlas: the default mode in the MSDL atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_overlay.html#visualizing-a-probabilistic-atlas-with-plot-prob-atlas">9.4.4.2.7. Visualizing a probabilistic atlas with plot_prob_atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_dim_plotting.html">9.4.4.2.8. Controlling the contrast of the background when plotting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_visualization.html">9.4.4.2.9. NeuroImaging volumes visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_carpet.html">9.4.4.2.10. Visualizing global patterns with a carpet plot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_haxby_masks.html">9.4.4.2.11. Plot Haxby masks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_surface_projection_strategies.html">9.4.4.2.12. Technical point: Illustration of the volume to surface sampling schemes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_plotting.html">9.4.4.2.13. Plotting tools in nilearn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_prob_atlas.html">9.4.4.2.14. Visualizing 4D probabilistic atlas maps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_surf_stat_map.html">9.4.4.2.15. Seed-based connectivity on the surface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_surf_atlas.html">9.4.4.2.16. Loading and plotting of a cortical surface atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_3d_map_to_surface_projection.html">9.4.4.2.17. Making a surface plot of a 3D statistical map</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_glass_brain_extensive.html">9.4.4.2.18. Glass brain plotting in nilearn (all options)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/01_plotting/plot_demo_more_plotting.html">9.4.4.2.19. More plotting tools from nilearn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/02_decoding/index.html">9.4.4.3. Decoding and predicting from brain images</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" role="switch" type="checkbox"/><label for="toctree-checkbox-20"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_stimuli.html">9.4.4.3.1. Show stimuli of Haxby et al. dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_mixed_gambles_frem.html">9.4.4.3.2. FREM on Jimura et al “mixed gambles” dataset.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_frem.html">9.4.4.3.3. Decoding with FREM: face vs house object recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_oasis_vbm_space_net.html">9.4.4.3.4. Voxel-Based Morphometry on Oasis dataset with Space-Net prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_anova_svm.html">9.4.4.3.5. Decoding with ANOVA + SVM: face vs house in the Haxby dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_searchlight_surface.html">9.4.4.3.6. Cortical surface-based searchlight decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_multiclass.html">9.4.4.3.7. The haxby dataset: different multi-class strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_searchlight.html">9.4.4.3.8. Searchlight analysis of face vs house recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_glm_decoding.html">9.4.4.3.9. Decoding of a dataset after GLM fit for signal extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_grid_search.html">9.4.4.3.10. Setting a parameter by cross-validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html">9.4.4.3.11. ROI-based decoding analysis in Haxby et al. dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_different_estimators.html">9.4.4.3.12. Different classifiers in decoding the Haxby dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_oasis_vbm.html">9.4.4.3.13. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_simulated_data.html">9.4.4.3.14. Example of pattern recognition on simulated data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_miyawaki_encoding.html">9.4.4.3.15. Encoding models for visual stimuli from Miyawaki et al. 2008</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/02_decoding/plot_miyawaki_reconstruction.html">9.4.4.3.16. Reconstruction of visual stimuli from Miyawaki et al. 2008</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/03_connectivity/index.html">9.4.4.4. Functional connectivity</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" role="switch" type="checkbox"/><label for="toctree-checkbox-21"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_probabilistic_atlas_extraction.html">9.4.4.4.1. Extracting signals of a probabilistic atlas of functional regions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_inverse_covariance_connectome.html">9.4.4.4.2. Computing a connectome with sparse inverse covariance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_simulated_connectome.html">9.4.4.4.3. Connectivity structure estimation on simulated data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_compare_decomposition.html">9.4.4.4.4. Deriving spatial maps from group fMRI data using ICA and Dictionary Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_seed_to_voxel_correlation.html">9.4.4.4.5. Producing single subject maps of seed-to-voxel correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_multi_subject_connectome.html">9.4.4.4.6. Group Sparse inverse covariance for multi-subject connectome</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_extract_regions_dictlearning_maps.html">9.4.4.4.7. Regions extraction using <span class="xref std std-term">Dictionary learning</span> and functional connectomes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_atlas_comparison.html">9.4.4.4.8. Comparing connectomes on different reference atlases</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_group_level_connectivity.html">9.4.4.4.9. Classification of age groups using functional connectivity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_signal_extraction.html">9.4.4.4.10. Extracting signals from a brain parcellation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_sphere_based_connectome.html">9.4.4.4.11. Extract signals on spheres and plot a connectome</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/03_connectivity/plot_data_driven_parcellations.html">9.4.4.4.12. Clustering methods to learn a brain parcellation from fMRI</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/04_glm_first_level/index.html">9.4.4.5. GLM: First level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" role="switch" type="checkbox"/><label for="toctree-checkbox-22"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_write_events_file.html">9.4.4.5.1. Generate an events.tsv file for the NeuroSpin localizer task</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_fixed_effects.html">9.4.4.5.2. Example of explicit fixed effects fMRI model fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_adhd_dmn.html">9.4.4.5.3. Default Mode Network extraction of AHDH dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_design_matrix.html">9.4.4.5.4. Examples of design matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_fir_model.html">9.4.4.5.5. Analysis of an fMRI dataset with a Finite Impule Response (FIR) model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_spm_multimodal_faces.html">9.4.4.5.6. Single-subject data (two sessions) in native space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_hrf.html">9.4.4.5.7. Example of MRI response functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_fiac_analysis.html">9.4.4.5.8. Simple example of two-session fMRI model fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_bids_features.html">9.4.4.5.9. First level analysis of a complete BIDS dataset from openneuro</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_predictions_residuals.html">9.4.4.5.10. Predicted time series and residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_localizer_surface_analysis.html">9.4.4.5.11. Example of surface-based first-level analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/04_glm_first_level/plot_first_level_details.html">9.4.4.5.12. Understanding parameters of the first-level model</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/05_glm_second_level/index.html">9.4.4.6. GLM : Second level analysis examples</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" role="switch" type="checkbox"/><label for="toctree-checkbox-23"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_design_matrix.html">9.4.4.6.1. Example of second level design matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_proportion_activated_voxels.html">9.4.4.6.2. Second-level fMRI model: true positive proportion in clusters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_thresholding.html">9.4.4.6.3. Statistical testing of a second-level analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_oasis.html">9.4.4.6.4. Voxel-Based Morphometry on Oasis dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_two_sample_test.html">9.4.4.6.5. Second-level fMRI model: two-sample test, unpaired and paired</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_one_sample_test.html">9.4.4.6.6. Second-level fMRI model: one sample test</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/05_glm_second_level/plot_second_level_association_test.html">9.4.4.6.7. Example of generic design in second-level models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/06_manipulating_images/index.html">9.4.4.7. Manipulating brain image volumes</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" role="switch" type="checkbox"/><label for="toctree-checkbox-24"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_negate_image.html">9.4.4.7.1. Negating an image with math_img</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_compare_mean_image.html">9.4.4.7.2. Comparing the means of 2 images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_smooth_mean_image.html">9.4.4.7.3. Smoothing an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_extract_rois_smith_atlas.html">9.4.4.7.4. Regions Extraction of Default Mode Networks using Smith Atlas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_extract_regions_labels_image.html">9.4.4.7.5. Breaking an atlas of labels in separated regions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_resample_to_template.html">9.4.4.7.6. Resample an image to a template</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_nifti_simple.html">9.4.4.7.7. Simple example of NiftiMasker use</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_nifti_labels_simple.html">9.4.4.7.8. Extracting signals from brain regions using the NiftiLabelsMasker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_extract_rois_statistical_maps.html">9.4.4.7.9. Region Extraction using a t-statistical map (3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_mask_computation.html">9.4.4.7.10. Understanding NiftiMasker and mask computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_affine_transformation.html">9.4.4.7.11. Visualization of affine resamplings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_roi_extraction.html">9.4.4.7.12. Computing a Region of Interest (ROI) mask manually</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/07_advanced/index.html">9.4.4.8. Advanced statistical analysis of brain images</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" role="switch" type="checkbox"/><label for="toctree-checkbox-25"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_ica_resting_state.html">9.4.4.8.1. Multivariate decompositions: Independent component analysis of fMRI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_localizer_simple_analysis.html">9.4.4.8.2. Massively univariate analysis of a calculation task from the Localizer dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_bids_analysis.html">9.4.4.8.3. BIDS dataset first and second level analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_age_group_prediction_cross_val.html">9.4.4.8.4. Functional connectivity predicts age group</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_neurovault_meta_analysis.html">9.4.4.8.5. NeuroVault meta-analysis of stop-go paradigm studies.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_surface_bids_analysis.html">9.4.4.8.6. Surface-based dataset first and second level analysis of a dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_localizer_mass_univariate_methods.html">9.4.4.8.7. Massively univariate analysis of a motor task from the Localizer dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_ica_neurovault.html">9.4.4.8.8. NeuroVault cross-study ICA maps.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_haxby_mass_univariate.html">9.4.4.8.9. Massively univariate analysis of face vs house recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/07_advanced/plot_advanced_decoding_scikit.html">9.4.4.8.10. Advanced decoding using scikit learn</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html">What’s new</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#dev">0.9.1.dev</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id100">0.9.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id204">0.8.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id309">0.8.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id414">0.7.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id518">0.7.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id623">0.6.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id726">0.6.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id829">0.6.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#rc">0.6.0rc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#b0">0.6.0b0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#a0">0.6.0a0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id948">0.5.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1051">0.5.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1155">0.5.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1159">0.5.0 rc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#beta">0.5.0 beta</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#alpha">0.5.0 alpha</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1272">0.4.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1374">0.4.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1476">0.4.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1581">0.3.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1685">0.3.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1689">0.3.0 beta</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1793">0.2.6</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1896">0.2.5.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id1900">0.2.5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2002">0.2.4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2104">0.2.3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2205">0.2.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#v0-2-1">0.2.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2312">0.2.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2417">0.1.4</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2518">0.1.3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2620">0.1.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2721">0.1.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes/whats_new.html#id2822">0.1.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development.html">Nilearn development process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../maintenance.html">Nilearn maintenance process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container"><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="from-neuroimaging-volumes-to-data-matrices-the-masker-objects">
<span id="masker-objects"></span><h1><span class="section-number">6.3. </span>From neuroimaging volumes to data matrices: the masker objects<a class="headerlink" href="#from-neuroimaging-volumes-to-data-matrices-the-masker-objects" title="Permalink to this headline">#</a></h1>
<p>This chapter introduces the maskers: objects that go from
neuroimaging volumes, on the disk or in memory, to data matrices, eg of
time series.</p>
<div class="contents local topic" id="chapters-contents">
<p class="topic-title"><strong>Chapters contents</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#the-concept-of-masker-objects" id="id2">The concept of “masker” objects</a></p></li>
<li><p><a class="reference internal" href="#niftimasker-applying-a-mask-to-load-time-series" id="id3"><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code>: applying a mask to load time-series</a></p></li>
<li><p><a class="reference internal" href="#extraction-of-signals-from-regions-niftilabelsmasker-niftimapsmasker" id="id4">Extraction of signals from regions: <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code></a></p></li>
<li><p><a class="reference internal" href="#extraction-of-signals-from-seeds-niftispheresmasker" id="id5">Extraction of signals from seeds: <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiSpheresMasker</span></code></a></p></li>
</ul>
</div>
<section id="the-concept-of-masker-objects">
<h2><a class="toc-backref" href="#id2"><span class="section-number">6.3.1. </span>The concept of “masker” objects</a><a class="headerlink" href="#the-concept-of-masker-objects" title="Permalink to this headline">#</a></h2>
<p>In any analysis, the first step is to load the data.
It is often convenient to apply some basic data
transformations and to turn the data in a 2D (samples x features) matrix,
where the samples could be different time points, and the features derived
from different voxels (e.g., restrict analysis to the ventral visual stream),
regions of interest (e.g., extract local signals from spheres/cubes), or
pre-specified networks (e.g., look at data from all voxels of a set of
network nodes). Think of masker objects as swiss-army knives for shaping
the raw neuroimaging data in 3D space into the units of observation
relevant for the research questions at hand.</p>
<p class="centered">
<strong><a class="reference internal" href="../_images/niimgs.jpg"><img alt="niimgs" src="../_images/niimgs.jpg" style="width: 367.0px; height: 163.5px;"/></a> <span style="padding: .5em; font-size: 400%">→</span> <a class="reference internal" href="../_images/feature_array.jpg"><img alt="arrays" src="../_images/feature_array.jpg" style="width: 115.14999999999999px; height: 167.29999999999998px;"/></a></strong></p><p>“masker” objects (found in module <a class="reference internal" href="../modules/reference.html#module-nilearn.maskers" title="nilearn.maskers"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nilearn.maskers</span></code></a>)
simplify these “data folding” steps that often precede the
statistical analysis.</p>
<p>Note that the masker objects may not cover all the image transformations
for specific tasks. Users who want to make some specific processing may
have to call <a class="reference internal" href="manipulating_images.html#preprocessing-functions"><span class="std std-ref">specific functions</span></a>
(modules <a class="reference internal" href="../modules/reference.html#module-nilearn.signal" title="nilearn.signal"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nilearn.signal</span></code></a>, <a class="reference internal" href="../modules/reference.html#module-nilearn.masking" title="nilearn.masking"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nilearn.masking</span></code></a>).</p>
<div class="line-block">
<div class="line"><br/></div>
</div>
<div class="topic">
<p class="topic-title"><strong>Advanced: Design philosophy of “Maskers”</strong></p>
<p>The design of these classes is similar to <a class="reference external" href="http://scikit-learn.org">scikit-learn</a>‘s transformers. First, objects are
initialized with some parameters guiding the transformation
(unrelated to the data). Then the <cite>fit()</cite> method should be called,
possibly specifying some data-related information (such as number of
images to process), to perform some initial computation (e.g.,
fitting a mask based on the data). Finally, <cite>transform()</cite> can be
called, with the data as argument, to perform some computation on
data themselves (e.g., extracting time series from images).</p>
</div>
</section>
<section id="niftimasker-applying-a-mask-to-load-time-series">
<span id="nifti-masker"></span><h2><a class="toc-backref" href="#id3"><span class="section-number">6.3.2. </span><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code>: applying a mask to load time-series</a><a class="headerlink" href="#niftimasker-applying-a-mask-to-load-time-series" title="Permalink to this headline">#</a></h2>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> is a powerful tool to load images and
extract voxel signals in the area defined by the mask.
It applies some basic preprocessing
steps with commonly used parameters as defaults.
But it is <em>very important</em> to look at your data to see the effects
of the preprocessings and validate them.</p>
<div class="topic">
<p class="topic-title"><strong>Advanced: scikit-learn Pipelines</strong></p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> is a <a class="reference external" href="http://scikit-learn.org">scikit-learn</a> compliant
transformer so that you can directly plug it into a <a class="reference external" href="http://scikit-learn.org/stable/modules/pipeline.html">scikit-learn
pipeline</a>.</p>
</div>
<section id="custom-data-loading-loading-only-the-first-100-time-points">
<h3><span class="section-number">6.3.2.1. </span>Custom data loading: loading only the first 100 time points<a class="headerlink" href="#custom-data-loading-loading-only-the-first-100-time-points" title="Permalink to this headline">#</a></h3>
<p>Suppose we want to restrict a dataset to the first 100 frames. Below, we load
a movie-watching dataset with <code class="xref py py-func docutils literal notranslate"><span class="pre">fetch_development_fmri()</span></code>, restrict it to 100 frames and
build a new niimg object that we can give to the masker. Although
possible, there is no need to save your data to a file to pass it to a
<code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code>. Simply use <code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.image.index_img</span></code> to apply a
slice and create a <a class="reference internal" href="input_output.html#niimg"><span class="std std-ref">Niimg</span></a> in memory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_development_fmri</span><span class="p">(</span><span class="n">n_subjects</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">epi_filename</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">func</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Restrict to 100 frames to speed up computation</span>
<span class="kn">from</span> <span class="nn">nilearn.image</span> <span class="kn">import</span> <span class="n">index_img</span>
<span class="n">epi_img</span> <span class="o">=</span> <span class="n">index_img</span><span class="p">(</span><span class="n">epi_filename</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>

</pre></div>
</div>
</section>
<section id="controlling-how-the-mask-is-computed-from-the-data">
<h3><span class="section-number">6.3.2.2. </span>Controlling how the mask is computed from the data<a class="headerlink" href="#controlling-how-the-mask-is-computed-from-the-data" title="Permalink to this headline">#</a></h3>
<p>In this section, we show how the masker object can compute a mask
automatically for subsequent statistical analysis.
On some datasets, the default algorithm may however perform poorly.
This is why it is very important to
<strong>always look at your data</strong> before and after feature
engineering using masker objects.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The full example described in this section can be found here:
<a class="reference internal" href="../auto_examples/06_manipulating_images/plot_mask_computation.html"><span class="doc">plot_mask_computation.py</span></a>.
It is also related to this example:
<a class="reference internal" href="../auto_examples/06_manipulating_images/plot_nifti_simple.html"><span class="doc">plot_nifti_simple.py</span></a>.</p>
</div>
<section id="visualizing-the-computed-mask">
<h4><span class="section-number">6.3.2.2.1. </span>Visualizing the computed mask<a class="headerlink" href="#visualizing-the-computed-mask" title="Permalink to this headline">#</a></h4>
<p>If a mask is not specified as an argument, <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> will try to
compute one from the provided neuroimaging data.
It is <em>very important</em> to verify the quality of the generated mask by visualization.
This allows to see whether it is suitable for your data and intended analyses.
Alternatively, the mask computation parameters can still be modified.
See the <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> documentation for a complete list of
mask computation parameters.</p>
<p>The mask can be retrieved and visualized from the <cite>mask_img_</cite> attribute
of the masker:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">masker</span> <span class="o">=</span> <span class="n">NiftiMasker</span><span class="p">()</span>
<span class="n">masker</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">miyawaki_filename</span><span class="p">)</span>

<span class="c1"># Plot the generated mask using the mask_img_ attribute</span>
<span class="n">plot_roi</span><span class="p">(</span><span class="n">masker</span><span class="o">.</span><span class="n">mask_img_</span><span class="p">,</span> <span class="n">miyawaki_mean_img</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s2">"Mask from already masked data"</span><span class="p">)</span>

<span class="c1">###############################################################################</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/06_manipulating_images/plot_mask_computation.html"><img alt="../_images/sphx_glr_plot_mask_computation_002.png" src="../_images/sphx_glr_plot_mask_computation_002.png" style="width: 264.0px; height: 104.0px;"/></a>
</figure>
<p>Alternatively, the mask can be visualized using the <cite>generate_report</cite>
method of the masker. The generated report can be viewed in a Jupyter notebook,
opened in a new browser tab using <cite>report.open_in_browser()</cite>,
or saved as a portable HTML file <cite>report.save_as_html(output_filepath)</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">masker</span> <span class="o">=</span> <span class="n">NiftiMasker</span><span class="p">(</span><span class="n">mask_strategy</span><span class="o">=</span><span class="s1">'epi'</span><span class="p">)</span>
<span class="n">masker</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">epi_img</span><span class="p">)</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">masker</span><span class="o">.</span><span class="n">generate_report</span><span class="p">()</span>
<span class="n">report</span>

<span class="c1">###############################################################################</span>
</pre></div>
</div>
<figure class="align-default">
<a class="reference external image-reference" href="../auto_examples/06_manipulating_images/plot_mask_computation.html"><img alt="../_images/niftimasker_report.png" src="../_images/niftimasker_report.png" style="width: 747.0px; height: 230.0px;"/></a>
</figure>
</section>
<section id="different-masking-strategies">
<h4><span class="section-number">6.3.2.2.2. </span>Different masking strategies<a class="headerlink" href="#different-masking-strategies" title="Permalink to this headline">#</a></h4>
<p>The <cite>mask_strategy</cite> argument controls how the mask is computed:</p>
<ul class="simple">
<li><p><cite>background</cite>: detects a continuous background</p></li>
<li><p><cite>epi</cite>: suitable for EPI images</p></li>
<li><p><cite>whole-brain-template</cite>: uses an MNI whole-brain template</p></li>
<li><p><cite>gm-template</cite>: uses an MNI grey-matter template</p></li>
<li><p><cite>wm-template</cite>: uses an MNI white-matter template</p></li>
</ul>
</section>
<section id="extra-mask-parameters-opening-cutoff">
<h4><span class="section-number">6.3.2.2.3. </span>Extra mask parameters: opening, cutoff…<a class="headerlink" href="#extra-mask-parameters-opening-cutoff" title="Permalink to this headline">#</a></h4>
<p>The underlying function is <code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.masking.compute_epi_mask</span></code>
called using the <cite>mask_args</cite> argument of the <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code>.
Controlling these arguments set the fine aspects of the mask. See the
functions documentation, or <a class="reference internal" href="../auto_examples/06_manipulating_images/plot_mask_computation.html"><span class="doc">the NiftiMasker example</span></a>.</p>
<figure class="align-default">
<a class="reference external image-reference" href="../auto_examples/06_manipulating_images/plot_mask_computation.html"><img alt="../_images/niftimasker_report_params.png" src="../_images/niftimasker_report_params.png" style="width: 742.0px; height: 227.0px;"/></a>
</figure>
</section>
</section>
<section id="common-data-preparation-steps-smoothing-filtering-resampling">
<span id="masker-preprocessing-steps"></span><h3><span class="section-number">6.3.2.3. </span>Common data preparation steps: smoothing, filtering, resampling<a class="headerlink" href="#common-data-preparation-steps-smoothing-filtering-resampling" title="Permalink to this headline">#</a></h3>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> comes with many parameters that enable data
preparation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">sklearn</span><span class="p">;</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">set_config</span><span class="p">(</span><span class="n">print_changed_only</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nilearn</span> <span class="kn">import</span> <span class="n">maskers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">masker</span> <span class="o">=</span> <span class="n">maskers</span><span class="o">.</span><span class="n">NiftiMasker</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">masker</span> 
<span class="go">NiftiMasker(detrend=False, dtype=None, high_pass=None,</span>
<span class="go">      high_variance_confounds=False, low_pass=None, mask_args=None,</span>
<span class="go">      mask_img=None, mask_strategy='background',</span>
<span class="go">      memory=Memory(location=None), memory_level=1, reports=True,</span>
<span class="go">      runs=None, smoothing_fwhm=None, standardize=False,</span>
<span class="go">      standardize_confounds=True, t_r=None,</span>
<span class="go">      target_affine=None, target_shape=None, verbose=0)</span>
</pre></div>
</div>
<p>The meaning of each parameter is described in the documentation of
<code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> (click on the name <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code>), here we
comment on the most important.</p>
<div class="topic">
<p class="topic-title"><strong>`dtype` argument</strong></p>
<p>Forcing your data to have a <cite>dtype</cite> of <strong>float32</strong> can help
save memory and is often a good-enough numerical precision.
You can force this cast by choosing <cite>dtype</cite> to be ‘auto’.
In the future this cast will be the default behaviour.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>If you do not want to use the <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> to perform these
simple operations on data, note that they can also be manually
accessed in nilearn such as in
<a class="reference internal" href="manipulating_images.html#preprocessing-functions"><span class="std std-ref">corresponding functions</span></a>.</p>
</div>
<section id="smoothing">
<h4><span class="section-number">6.3.2.3.1. </span>Smoothing<a class="headerlink" href="#smoothing" title="Permalink to this headline">#</a></h4>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> can apply Gaussian spatial smoothing to the
neuroimaging data, useful to fight noise or for inter-individual
differences in neuroanatomy. It is achieved by specifying the
<a class="reference internal" href="../glossary.html#term-FWHM"><span class="xref std std-term">full-width half maximum</span></a> (<a class="reference internal" href="../glossary.html#term-FWHM"><span class="xref std std-term">FWHM</span></a>; in millimeter
scale) with the <cite>smoothing_fwhm</cite> parameter. Anisotropic filtering
is also possible by passing 3 scalars <code class="docutils literal notranslate"><span class="pre">(x,</span> <span class="pre">y,</span> <span class="pre">z)</span></code>, the
<a class="reference internal" href="../glossary.html#term-FWHM"><span class="xref std std-term">FWHM</span></a> along the x, y, and z direction.</p>
<p>The underlying function handles properly non-cubic <a class="reference internal" href="../glossary.html#term-voxel"><span class="xref std std-term">voxels</span></a>
by scaling the given widths appropriately.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.image.smooth_img</span></code></p>
</div>
</section>
<section id="temporal-filtering-and-confound-removal">
<span id="temporal-filtering"></span><h4><span class="section-number">6.3.2.3.2. </span>Temporal Filtering and confound removal<a class="headerlink" href="#temporal-filtering-and-confound-removal" title="Permalink to this headline">#</a></h4>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> can also improve aspects of temporal data
properties, before conversion to voxel signals.</p>
<ul class="simple">
<li><p><strong>Standardization</strong>. Parameter <code class="docutils literal notranslate"><span class="pre">standardize</span></code>: Signals can be
standardized (scaled to unit variance).</p></li>
<li><p><strong>Frequency filtering</strong>. Low-pass and high-pass filters can be used to
remove artifacts. Parameters: <code class="docutils literal notranslate"><span class="pre">high_pass</span></code> and <code class="docutils literal notranslate"><span class="pre">low_pass</span></code>, specified
in Hz (note that you must specific the sampling rate in seconds with
the <code class="docutils literal notranslate"><span class="pre">t_r</span></code> parameter: <code class="docutils literal notranslate"><span class="pre">loss_pass=.5,</span> <span class="pre">t_r=2.1</span></code>).</p></li>
<li><p><strong>Confound removal</strong>. Two ways of removing confounds are provided: simple
detrending or using prespecified confounds, such as behavioral or movement
information.</p>
<ul>
<li><p>Linear trends can be removed by activating the <cite>detrend</cite> parameter.
This accounts for slow (as opposed to abrupt or transient) changes
in voxel values along a series of brain images that are unrelated to the
signal of interest (e.g., the neural correlates of cognitive tasks).
It is not activated by default in <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> but is recommended
in almost all scenarios.</p></li>
<li><p>More complex confounds, measured during the acquision, can be removed
by passing them to <code class="xref py py-meth docutils literal notranslate"><span class="pre">NiftiMasker.transform</span></code>. If the dataset
provides a confounds file, just pass its path to the masker. For
<a class="reference internal" href="../glossary.html#term-fMRIPrep"><span class="xref std std-term">fMRIPrep</span></a> outputs, one can use
<code class="xref py py-func docutils literal notranslate"><span class="pre">load_confounds</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">load_confounds_strategy</span></code> to select
confound variables with some basic sanity check based on
<a class="reference internal" href="../glossary.html#term-fMRIPrep"><span class="xref std std-term">fMRIPrep</span></a> documentation.</p></li>
</ul>
</li>
</ul>
<div class="green topic">
<p class="topic-title"><strong>Exercise</strong></p>
<p>You can, more as a training than as an exercise, try to play with
the parameters in
<span class="xref std std-ref">sphx_glr_auto_examples_plot_decoding_tutorial.py</span>.
Try to enable detrending and run the script:
does it have a big impact on the result?</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please see the usage example of
<code class="xref py py-func docutils literal notranslate"><span class="pre">load_confounds</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">load_confounds_strategy</span></code> in
<a class="reference internal" href="../auto_examples/03_connectivity/plot_signal_extraction.html"><span class="doc">plot_signal_extraction.py</span></a>.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.signal.clean</span></code></p>
</div>
</section>
<section id="resampling-resizing-and-changing-resolutions-of-images">
<h4><span class="section-number">6.3.2.3.3. </span>Resampling: resizing and changing resolutions of images<a class="headerlink" href="#resampling-resizing-and-changing-resolutions-of-images" title="Permalink to this headline">#</a></h4>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code> and many similar classes enable resampling
(recasting of images into different resolutions and transformations of
brain voxel data). Two parameters control resampling:</p>
<ul class="simple">
<li><p><cite>target_affine</cite> to resample (resize, rotate…) images in order to match
the spatial configuration defined by the new affine (i.e., matrix
transforming from voxel space into world space).</p></li>
<li><p>Additionally, a <cite>target_shape</cite> can be used to resize images
(i.e., cropping or padding with zeros) to match an expected data
image dimensions (shape composed of x, y, and z).</p></li>
</ul>
<p>How to combine these parameter to obtain the specific resampling desired
is explained in details in <a class="reference internal" href="manipulating_images.html#resampling"><span class="std std-ref">Resampling images</span></a>.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.image.resample_img</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.image.resample_to_img</span></code></p>
</div>
</section>
</section>
<section id="inverse-transform-unmasking-data">
<span id="unmasking-step"></span><h3><span class="section-number">6.3.2.4. </span>Inverse transform: unmasking data<a class="headerlink" href="#inverse-transform-unmasking-data" title="Permalink to this headline">#</a></h3>
<p>Once voxel signals have been processed, the result can be visualized as
images after unmasking (masked-reduced data transformed back into
the original whole-brain space). This step is present in many
<a class="reference external" href="https://matplotlib.org/stable/gallery/index.html#examples-index" title="(in Matplotlib v3.5.1)"><span class="xref std std-ref">examples</span></a> provided in nilearn. Below you will find
an excerpt of <a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_anova_svm.html#sphx-glr-auto-examples-02-decoding-plot-haxby-anova-svm-py"><span class="std std-ref">the example performing Anova-SVM on the Haxby data</span></a>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># :class:`nilearn.plotting.plot_stat_map`</span>
<span class="n">weight_img</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">coef_img_</span><span class="p">[</span><span class="s1">'face'</span><span class="p">]</span>
<span class="kn">from</span> <span class="nn">nilearn.plotting</span> <span class="kn">import</span> <span class="n">plot_stat_map</span><span class="p">,</span> <span class="n">show</span>
<span class="n">plot_stat_map</span><span class="p">(</span><span class="n">weight_img</span><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><span class="n">haxby_dataset</span><span class="o">.</span><span class="n">anat</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">'SVM weights'</span><span class="p">)</span>

<span class="n">show</span><span class="p">()</span>
<span class="c1">#############################################################################</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br/></div>
</div>
<div class="topic">
<p class="topic-title"><strong>Examples to better understand the NiftiMasker</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_nifti_simple.html#sphx-glr-auto-examples-06-manipulating-images-plot-nifti-simple-py"><span class="std std-ref">Simple example of NiftiMasker use</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/06_manipulating_images/plot_mask_computation.html#sphx-glr-auto-examples-06-manipulating-images-plot-mask-computation-py"><span class="std std-ref">Understanding NiftiMasker and mask computation</span></a></p></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br/></div>
</div>
</section>
</section>
<section id="extraction-of-signals-from-regions-niftilabelsmasker-niftimapsmasker">
<span id="region"></span><h2><a class="toc-backref" href="#id4"><span class="section-number">6.3.3. </span>Extraction of signals from regions: <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code></a><a class="headerlink" href="#extraction-of-signals-from-regions-niftilabelsmasker-niftimapsmasker" title="Permalink to this headline">#</a></h2>
<p>The purpose of <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code> is to
compute signals from regions containing many voxels. They make it easy to get
these signals once you have an atlas or a parcellation into brain regions.</p>
<section id="regions-definition">
<h3><span class="section-number">6.3.3.1. </span>Regions definition<a class="headerlink" href="#regions-definition" title="Permalink to this headline">#</a></h3>
<p>Nilearn understands two different ways of defining regions, which are called
labels and maps, handled by <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code>, respectively.</p>
<ul class="simple">
<li><p>labels: a single region is defined as the set of all the voxels that have a
common label (e.g., anatomical brain region definitions as integers)
in the region definition array. The set of
regions is defined by a single 3D array, containing a voxel-wise
dictionary of label numbers that denote what
region a given voxel belongs to. This technique has a big advantage: the
required memory load is independent of the number of regions, allowing
for a large number of regions. On the other hand, there are
several disadvantages: regions cannot spatially overlap
and are represented in a binary present/nonpresent coding (no weighting).</p></li>
<li><p>maps: a single region is defined as the set of all the voxels that have a
non-zero weight. A set of regions is thus defined by a set of 3D images (or a
single 4D image), one 3D image per region (as opposed to all regions in a
single 3D image such as for labels, cf. above).
While these defined weighted regions can exhibit spatial
overlap (as opposed to labels), storage cost scales linearly with the
number of regions. Handling a large number (e.g., thousands)
of regions will prove difficult with this data transformation of
whole-brain voxel data into weighted region-wise data.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These usage are illustrated in the section <a class="reference internal" href="../connectivity/functional_connectomes.html#functional-connectomes"><span class="std std-ref">Extracting times series to build a functional connectome</span></a>.</p>
</div>
</section>
<section id="niftilabelsmasker-usage">
<h3><span class="section-number">6.3.3.2. </span><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code> Usage<a class="headerlink" href="#niftilabelsmasker-usage" title="Permalink to this headline">#</a></h3>
<p>Usage of <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code> is similar to that of
<code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code>. The main difference is that it requires a labels image
instead of a set of maps as input.</p>
<p>The <cite>background_label</cite> keyword of <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code> deserves
some explanation. The voxels that correspond to the brain or a region
of interest in an fMRI image do not fill the entire image.
Consequently, in the labels image, there must be a label value that corresponds
to “outside” the brain (for which no signal should be extracted).
By default, this label is set to zero in nilearn (referred to as “background”).
Should some non-zero value encoding be necessary, it is possible
to change the background value with the <cite>background_label</cite> keyword.</p>
<div class="topic">
<p class="topic-title"><strong>Examples</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/03_connectivity/plot_signal_extraction.html#sphx-glr-auto-examples-03-connectivity-plot-signal-extraction-py"><span class="std std-ref">Extracting signals from a brain parcellation</span></a></p></li>
</ul>
</div>
</section>
<section id="niftimapsmasker-usage">
<h3><span class="section-number">6.3.3.3. </span><code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code> Usage<a class="headerlink" href="#niftimapsmasker-usage" title="Permalink to this headline">#</a></h3>
<p>This atlas defines its regions using maps. The path to the corresponding
file is given in the <cite>maps_img</cite> argument.</p>
<p>One important thing that happens transparently during the execution of
<code class="xref py py-meth docutils literal notranslate"><span class="pre">NiftiMasker.fit_transform</span></code> is resampling. Initially, the images
and the atlas do typically not have the same shape nor the same affine.
Casting them into the same format is required for successful signal extraction
The keyword argument <cite>resampling_target</cite> specifies which format
(i.e., dimensions and affine) the data should be resampled to.
See the reference documentation for <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code> for every
possible option.</p>
<div class="topic">
<p class="topic-title"><strong>Examples</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/03_connectivity/plot_probabilistic_atlas_extraction.html#sphx-glr-auto-examples-03-connectivity-plot-probabilistic-atlas-extraction-py"><span class="std std-ref">Extracting signals of a probabilistic atlas of functional regions</span></a></p></li>
</ul>
</div>
</section>
</section>
<section id="extraction-of-signals-from-seeds-niftispheresmasker">
<h2><a class="toc-backref" href="#id5"><span class="section-number">6.3.4. </span>Extraction of signals from seeds: <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiSpheresMasker</span></code></a><a class="headerlink" href="#extraction-of-signals-from-seeds-niftispheresmasker" title="Permalink to this headline">#</a></h2>
<p>The purpose of <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiSpheresMasker</span></code> is to compute signals from
seeds containing voxels in spheres. It makes it easy to get these signals once
you have a list of coordinates.
A single seed is a sphere defined by the radius (in millimeters) and the
coordinates (typically MNI or TAL) of its center.</p>
<p>Using <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiSpheresMasker</span></code> needs to define a list of coordinates.
<cite>seeds</cite> argument takes a list of 3D coordinates (tuples) of the spheres centers,
they should be in the same space as the images.
Seeds can overlap spatially and are represented in a binary present/nonpresent
coding (no weighting).
Below is an example of a coordinates list of four seeds from the default mode network:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dmn_coords</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">52</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">46</span><span class="p">,</span> <span class="o">-</span><span class="mi">68</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">46</span><span class="p">,</span> <span class="o">-</span><span class="mi">68</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">)]</span>
</pre></div>
</div>
<p><cite>radius</cite> is an optional argument that takes a real value in millimeters.
If no value is given for the <cite>radius</cite> argument, the single voxel at the given
seed position is used.</p>
<div class="topic">
<p class="topic-title"><strong>Examples</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/03_connectivity/plot_sphere_based_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-sphere-based-connectome-py"><span class="std std-ref">Extract signals on spheres and plot a connectome</span></a></p></li>
</ul>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../building_blocks/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title"><span class="section-number">7. </span>Advanced usage: manual pipelines and scaling up</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="manipulating_images.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title"><span class="section-number">6.2. </span>Manipulating images: resampling, smoothing, masking, ROIs…</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; The nilearn developers 2010-2022
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">6.3. From neuroimaging volumes to data matrices: the masker objects</a><ul>
<li><a class="reference internal" href="#the-concept-of-masker-objects">6.3.1. The concept of “masker” objects</a></li>
<li><a class="reference internal" href="#niftimasker-applying-a-mask-to-load-time-series">6.3.2. <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMasker</span></code>: applying a mask to load time-series</a><ul>
<li><a class="reference internal" href="#custom-data-loading-loading-only-the-first-100-time-points">6.3.2.1. Custom data loading: loading only the first 100 time points</a></li>
<li><a class="reference internal" href="#controlling-how-the-mask-is-computed-from-the-data">6.3.2.2. Controlling how the mask is computed from the data</a><ul>
<li><a class="reference internal" href="#visualizing-the-computed-mask">6.3.2.2.1. Visualizing the computed mask</a></li>
<li><a class="reference internal" href="#different-masking-strategies">6.3.2.2.2. Different masking strategies</a></li>
<li><a class="reference internal" href="#extra-mask-parameters-opening-cutoff">6.3.2.2.3. Extra mask parameters: opening, cutoff…</a></li>
</ul>
</li>
<li><a class="reference internal" href="#common-data-preparation-steps-smoothing-filtering-resampling">6.3.2.3. Common data preparation steps: smoothing, filtering, resampling</a><ul>
<li><a class="reference internal" href="#smoothing">6.3.2.3.1. Smoothing</a></li>
<li><a class="reference internal" href="#temporal-filtering-and-confound-removal">6.3.2.3.2. Temporal Filtering and confound removal</a></li>
<li><a class="reference internal" href="#resampling-resizing-and-changing-resolutions-of-images">6.3.2.3.3. Resampling: resizing and changing resolutions of images</a></li>
</ul>
</li>
<li><a class="reference internal" href="#inverse-transform-unmasking-data">6.3.2.4. Inverse transform: unmasking data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#extraction-of-signals-from-regions-niftilabelsmasker-niftimapsmasker">6.3.3. Extraction of signals from regions: <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code></a><ul>
<li><a class="reference internal" href="#regions-definition">6.3.3.1. Regions definition</a></li>
<li><a class="reference internal" href="#niftilabelsmasker-usage">6.3.3.2. <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiLabelsMasker</span></code> Usage</a></li>
<li><a class="reference internal" href="#niftimapsmasker-usage">6.3.3.3. <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiMapsMasker</span></code> Usage</a></li>
</ul>
</li>
<li><a class="reference internal" href="#extraction-of-signals-from-seeds-niftispheresmasker">6.3.4. Extraction of signals from seeds: <code class="xref py py-class docutils literal notranslate"><span class="pre">NiftiSpheresMasker</span></code></a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/design-tabs.js"></script>
    </body>
</html>